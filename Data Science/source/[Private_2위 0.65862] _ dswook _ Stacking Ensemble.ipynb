{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 변수 설명 및 대회 전략\n+ GitHub README에 다 적어놨습니다.\n+ 중복데이터 처리를 어떻게 해야할지 고민을 하였습니다.\n+ Feature Engineering보단 Catboost의 Categorical 데이터에 관한 하이퍼파라미터 설정에 포커스를 맞췄습니다.\n+ [GitHub](https://github.com/ds-wook/PredictCreditCardDelinquency)\n\n# 참고한 자료\n+ [Hayo 님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2519?page=1&dtype=recent)\n+ [rollcake님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2526?page=1&dtype=recent)\n+ [datu님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2515?page=2&dtype=recent)\n+ [catboost 논문](https://arxiv.org/pdf/1706.09516.pdf)\n+ [catboost 활용 논문](https://arxiv.org/abs/2104.07553)","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y typing # this should avoid  AttributeError: type object 'Callable' has no attribute '_abc_registry'\n!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade","metadata":{"execution":{"iopub.status.busy":"2021-05-24T03:17:27.333131Z","iopub.execute_input":"2021-05-24T03:17:27.333498Z","iopub.status.idle":"2021-05-24T03:17:57.518299Z","shell.execute_reply.started":"2021-05-24T03:17:27.333420Z","shell.execute_reply":"2021-05-24T03:17:57.517366Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping typing as it is not installed.\u001b[0m\nCollecting pytorch_tabnet\n  Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to /tmp/pip-install-2ke2ioc2/pytorch-tabnet_05b3f8a626bb4a6994d213e123b81bcb\n  Running command git clone -q https://github.com/dreamquark-ai/tabnet.git /tmp/pip-install-2ke2ioc2/pytorch-tabnet_05b3f8a626bb4a6994d213e123b81bcb\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.19.5)\nRequirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (0.24.1)\nRequirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.5.4)\nRequirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.7.0)\nRequirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (4.59.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch_tabnet) (2.1.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (0.6)\nBuilding wheels for collected packages: pytorch-tabnet\n  Building wheel for pytorch-tabnet (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-tabnet: filename=pytorch_tabnet-3.1.1-py3-none-any.whl size=39326 sha256=dbf370437df193bcfe8cb3ee932ec034b2624d8b3cea2c676b2862d1c401b1dc\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xa0gkqiu/wheels/a6/8e/aa/6f5ef6a2e389c8b5f7ea1c74bbb03ece8773b03c2b8955c334\nSuccessfully built pytorch-tabnet\nInstalling collected packages: pytorch-tabnet\nSuccessfully installed pytorch-tabnet-3.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Dict, Tuple, Union, List\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom pytorch_tabnet.multitask import TabNetMultiTaskClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-24T03:17:57.521745Z","iopub.execute_input":"2021-05-24T03:17:57.522016Z","iopub.status.idle":"2021-05-24T03:18:01.889106Z","shell.execute_reply.started":"2021-05-24T03:17:57.521975Z","shell.execute_reply":"2021-05-24T03:18:01.888260Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Dataset\n+ catboost가 이 대회의 핵심 알고리즘이라고 생각하여 categorical한 변수를 만드는게 중요하다고 생각하여 따로 함수를 만듬\n+ 다른 알고리즘들은 categorical한 변수보다 numeric한 변수 처리에 초점을 맞춤","metadata":{}},{"cell_type":"code","source":"def category_income(data: pd.DataFrame) -> pd.DataFrame:\n    data[\"income_total\"] = data[\"income_total\"] / 10000\n    conditions = [\n        (data[\"income_total\"].le(18)),\n        (data[\"income_total\"].gt(18) & data[\"income_total\"].le(33)),\n        (data[\"income_total\"].gt(33) & data[\"income_total\"].le(49)),\n        (data[\"income_total\"].gt(49) & data[\"income_total\"].le(64)),\n        (data[\"income_total\"].gt(64) & data[\"income_total\"].le(80)),\n        (data[\"income_total\"].gt(80) & data[\"income_total\"].le(95)),\n        (data[\"income_total\"].gt(95) & data[\"income_total\"].le(111)),\n        (data[\"income_total\"].gt(111) & data[\"income_total\"].le(126)),\n        (data[\"income_total\"].gt(126) & data[\"income_total\"].le(142)),\n        (data[\"income_total\"].gt(142)),\n    ]\n    choices = [i for i in range(10)]\n\n    data[\"income_total\"] = np.select(conditions, choices)\n    return data\n\n\ndef load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n    path = \"../input/predictcreditcarddelinquency/\"\n    train = pd.read_csv(path + \"train.csv\")\n    train = train.drop([\"index\"], axis=1)\n    train.fillna(\"NAN\", inplace=True)\n\n    test = pd.read_csv(path + \"test.csv\")\n    test = test.drop([\"index\"], axis=1)\n    test.fillna(\"NAN\", inplace=True)\n\n    # absolute\n    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n\n    # DAYS_BIRTH\n    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n    )\n    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n    )\n    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n    )\n    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n    )\n    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n\n    # Age\n    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n\n    # DAYS_EMPLOYED\n    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n\n    # EMPLOYED\n    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n\n    # before_EMPLOYED\n    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n\n    # gender_car_reality\n    train[\"user_code\"] = (\n        train[\"gender\"].astype(str)\n        + \"_\"\n        + train[\"car\"].astype(str)\n        + \"_\"\n        + train[\"reality\"].astype(str)\n    )\n    test[\"user_code\"] = (\n        test[\"gender\"].astype(str)\n        + \"_\"\n        + test[\"car\"].astype(str)\n        + \"_\"\n        + test[\"reality\"].astype(str)\n    )\n\n    del_cols = [\n        \"gender\",\n        \"car\",\n        \"reality\",\n        \"email\",\n        \"child_num\",\n        \"DAYS_BIRTH\",\n        \"DAYS_EMPLOYED\",\n    ]\n    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n    train.drop(del_cols, axis=1, inplace=True)\n    test.drop(del_cols, axis=1, inplace=True)\n\n    cat_cols = [\n        \"income_type\",\n        \"edu_type\",\n        \"family_type\",\n        \"house_type\",\n        \"occyp_type\",\n        \"user_code\",\n    ]\n\n    for col in cat_cols:\n        label_encoder = LabelEncoder()\n        label_encoder = label_encoder.fit(train[col])\n        train[col] = label_encoder.transform(train[col])\n        test[col] = label_encoder.transform(test[col])\n\n    return train, test\n\n\ndef cat_load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n    path = \"../input/predictcreditcarddelinquency/\"\n    train = pd.read_csv(path + \"train.csv\")\n    train = train.drop([\"index\"], axis=1)\n    train.fillna(\"NAN\", inplace=True)\n\n    test = pd.read_csv(path + \"test.csv\")\n    test = test.drop([\"index\"], axis=1)\n    test.fillna(\"NAN\", inplace=True)\n\n    # absolute\n    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n\n    # income_total\n    train = category_income(train)\n    test = category_income(test)\n\n    # DAYS_BIRTH\n    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n    )\n    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n    )\n    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n    )\n    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n    )\n    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n\n    # Age\n    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n\n    # DAYS_EMPLOYED\n    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n\n    # EMPLOYED\n    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n\n    # before_EMPLOYED\n    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n    )\n    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n    )\n    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n\n    # gender_car_reality\n    train[\"user_code\"] = (\n        train[\"gender\"].astype(str)\n        + \"_\"\n        + train[\"car\"].astype(str)\n        + \"_\"\n        + train[\"reality\"].astype(str)\n    )\n    test[\"user_code\"] = (\n        test[\"gender\"].astype(str)\n        + \"_\"\n        + test[\"car\"].astype(str)\n        + \"_\"\n        + test[\"reality\"].astype(str)\n    )\n\n    del_cols = [\n        \"gender\",\n        \"car\",\n        \"reality\",\n        \"email\",\n        \"child_num\",\n        \"DAYS_BIRTH\",\n        \"DAYS_EMPLOYED\",\n    ]\n    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n    train.drop(del_cols, axis=1, inplace=True)\n    test.drop(del_cols, axis=1, inplace=True)\n\n    cat_cols = [\n        \"income_type\",\n        \"edu_type\",\n        \"family_type\",\n        \"house_type\",\n        \"occyp_type\",\n        \"user_code\",\n    ]\n\n    for col in cat_cols:\n        label_encoder = LabelEncoder()\n        label_encoder = label_encoder.fit(train[col])\n        train[col] = label_encoder.transform(train[col])\n        test[col] = label_encoder.transform(test[col])\n\n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-05-24T03:18:01.890877Z","iopub.execute_input":"2021-05-24T03:18:01.891168Z","iopub.status.idle":"2021-05-24T03:18:01.947487Z","shell.execute_reply.started":"2021-05-24T03:18:01.891140Z","shell.execute_reply":"2021-05-24T03:18:01.946520Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# Catboost\ndef stratified_kfold_cat(\n    params: Dict[str, Union[int, float, str, List[str]]],\n    n_fold: int,\n    X: pd.DataFrame,\n    y: pd.DataFrame,\n    X_test: pd.DataFrame,\n) -> Tuple[np.ndarray, np.ndarray]:\n    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n    splits = folds.split(X, y)\n    cat_oof = np.zeros((X.shape[0], 3))\n    cat_preds = np.zeros((X_test.shape[0], 3))\n    cat_cols = [c for c in X.columns if X[c].dtypes == \"int64\"]\n\n    for fold, (train_idx, valid_idx) in enumerate(splits):\n        print(f\"============ Fold {fold} ============\\n\")\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n        valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols)\n\n        model = CatBoostClassifier(**params)\n\n        model.fit(\n            train_data,\n            eval_set=valid_data,\n            early_stopping_rounds=100,\n            use_best_model=True,\n            verbose=100,\n        )\n\n        cat_oof[valid_idx] = model.predict_proba(X_valid)\n        cat_preds += model.predict_proba(X_test) / n_fold\n\n    log_score = log_loss(y, cat_oof)\n    print(f\"Log Loss Score: {log_score:.5f}\\n\")\n    return cat_oof, cat_preds\n\n\n# Light GBM\ndef stratified_kfold_lgbm(\n    params: Dict[str, Union[int, float, str]],\n    n_fold: int,\n    X: pd.DataFrame,\n    y: pd.DataFrame,\n    X_test: pd.DataFrame,\n) -> Tuple[np.ndarray, np.ndarray]:\n    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n    splits = folds.split(X, y)\n    lgb_oof = np.zeros((X.shape[0], 3))\n    lgb_preds = np.zeros((X_test.shape[0], 3))\n\n    for fold, (train_idx, valid_idx) in enumerate(splits):\n        print(f\"============ Fold {fold} ============\\n\")\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        pre_model = LGBMClassifier(**params)\n\n        pre_model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n            early_stopping_rounds=100,\n            verbose=100,\n        )\n        params2 = params.copy()\n        params2[\"learning_rate\"] = params[\"learning_rate\"] * 0.1\n\n        model = LGBMClassifier(**params2)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n            early_stopping_rounds=100,\n            verbose=100,\n            init_model=pre_model,\n        )\n        lgb_oof[valid_idx] = model.predict_proba(X_valid)\n        lgb_preds += model.predict_proba(X_test) / n_fold\n\n    log_score = log_loss(y, lgb_oof)\n    print(f\"Log Loss Score: {log_score:.5f}\")\n\n    return lgb_oof, lgb_preds\n\n\n# XGB\ndef stratified_kfold_xgb(\n    params: Dict[str, Union[int, float, str]],\n    n_fold: int,\n    X: pd.DataFrame,\n    y: pd.DataFrame,\n    X_test: pd.DataFrame,\n) -> Tuple[np.ndarray, np.ndarray]:\n\n    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n    splits = folds.split(X, y)\n    xgb_oof = np.zeros((X.shape[0], 3))\n    xgb_preds = np.zeros((X_test.shape[0], 3))\n\n    for fold, (train_idx, valid_idx) in enumerate(splits):\n        print(f\"============ Fold {fold} ============\\n\")\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n        model = XGBClassifier(**params)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n            early_stopping_rounds=100,\n            verbose=100,\n        )\n\n        xgb_oof[valid_idx] = model.predict_proba(X_valid)\n        xgb_preds += model.predict_proba(X_test) / n_fold\n\n    log_score = log_loss(y, xgb_oof)\n    print(f\"Log Loss Score: {log_score:.5f}\")\n\n    return xgb_oof, xgb_preds\n\n\n# Random Foreset\ndef stratified_kfold_rf(\n    params: Dict[str, Union[int, float, str, bool]],\n    n_fold: int,\n    X: pd.DataFrame,\n    y: pd.DataFrame,\n    X_test: pd.DataFrame,\n) -> Tuple[np.ndarray, np.ndarray]:\n\n    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n    splits = folds.split(X, y)\n    rf_oof = np.zeros((X.shape[0], 3))\n    rf_preds = np.zeros((X_test.shape[0], 3))\n\n    for fold, (train_idx, valid_idx) in enumerate(splits):\n        print(f\"============ Fold {fold} ============\\n\")\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        model = RandomForestClassifier(**params)\n        model.fit(\n            X_train,\n            y_train,\n        )\n\n        rf_oof[valid_idx] = model.predict_proba(X_valid)\n        rf_preds += model.predict_proba(X_test) / n_fold\n        print(f\"Log Loss Score: {log_loss(y_valid, rf_oof[valid_idx]):.5f}\")\n\n    log_score = log_loss(y, rf_oof)\n    print(f\"Log Loss Score: {log_score:.5f}\")\n\n    return rf_oof, rf_preds","metadata":{"execution":{"iopub.status.busy":"2021-05-24T03:18:01.949262Z","iopub.execute_input":"2021-05-24T03:18:01.949594Z","iopub.status.idle":"2021-05-24T03:18:01.977430Z","shell.execute_reply.started":"2021-05-24T03:18:01.949559Z","shell.execute_reply":"2021-05-24T03:18:01.976562Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Cat Train\n+ catboost 같은 경우 categorical한 feature를 고정시켜주는게 핵심이라고 생각하여 하이퍼파라미터튜닝에 신경을 씀\n+ 하이퍼파라미터튜닝은 optuna 라이브러리로 했음","metadata":{}},{"cell_type":"code","source":"train_cat, test_cat = cat_load_dataset()\nX = train_cat.drop(\"credit\", axis=1)\ny = train_cat[\"credit\"]\n\nX_test = test_cat.copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T03:18:01.978709Z","iopub.execute_input":"2021-05-24T03:18:01.979135Z","iopub.status.idle":"2021-05-24T03:18:02.424836Z","shell.execute_reply.started":"2021-05-24T03:18:01.979068Z","shell.execute_reply":"2021-05-24T03:18:02.424030Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cat_params = {\n    \"learning_rate\": 0.026612467217016746,\n    \"l2_leaf_reg\": 0.3753065117824262,\n    \"max_depth\": 8,\n    \"bagging_temperature\": 1,\n    \"min_data_in_leaf\": 57,\n    \"max_bin\": 494,\n    \"random_state\": 42,\n    \"eval_metric\": \"MultiClass\",\n    \"loss_function\": \"MultiClass\",\n    \"od_type\": \"Iter\",\n    \"od_wait\": 500,\n    \"iterations\": 10000,\n    \"cat_features\": [\n        \"income_total\",\n        \"income_type\",\n        \"edu_type\",\n        \"family_type\",\n        \"house_type\",\n        \"FLAG_MOBIL\",\n        \"work_phone\",\n        \"phone\",\n        \"occyp_type\",\n        \"begin_month\",\n        \"DAYS_BIRTH_month\",\n        \"DAYS_BIRTH_week\",\n        \"Age\",\n        \"DAYS_EMPLOYED_month\",\n        \"DAYS_EMPLOYED_week\",\n        \"before_EMPLOYED\",\n        \"before_EMPLOYED_month\",\n        \"before_EMPLOYED_week\",\n        \"user_code\",\n    ],\n}\n\ncat_oof, cat_preds = stratified_kfold_cat(cat_params, 10, X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T03:18:02.426103Z","iopub.execute_input":"2021-05-24T03:18:02.426466Z","iopub.status.idle":"2021-05-24T04:38:25.616125Z","shell.execute_reply.started":"2021-05-24T03:18:02.426429Z","shell.execute_reply":"2021-05-24T04:38:25.615056Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"============ Fold 0 ============\n\n0:\tlearn: 1.0832640\ttest: 1.0829576\tbest: 1.0829576 (0)\ttotal: 301ms\tremaining: 50m 14s\n100:\tlearn: 0.7323907\ttest: 0.6929501\tbest: 0.6929501 (100)\ttotal: 37.1s\tremaining: 1h 37s\n200:\tlearn: 0.7058060\ttest: 0.6735975\tbest: 0.6735975 (200)\ttotal: 1m 27s\tremaining: 1h 10m 51s\n300:\tlearn: 0.6885144\ttest: 0.6706689\tbest: 0.6706689 (300)\ttotal: 2m 22s\tremaining: 1h 16m 18s\n400:\tlearn: 0.6701975\ttest: 0.6682080\tbest: 0.6681988 (388)\ttotal: 3m 16s\tremaining: 1h 18m 16s\n500:\tlearn: 0.6494751\ttest: 0.6664689\tbest: 0.6662605 (486)\ttotal: 4m 11s\tremaining: 1h 19m 23s\n600:\tlearn: 0.6290599\ttest: 0.6657848\tbest: 0.6657848 (600)\ttotal: 5m 6s\tremaining: 1h 19m 56s\n700:\tlearn: 0.6080674\ttest: 0.6648291\tbest: 0.6648201 (698)\ttotal: 6m 2s\tremaining: 1h 20m 7s\n800:\tlearn: 0.5895904\ttest: 0.6646412\tbest: 0.6643652 (761)\ttotal: 6m 58s\tremaining: 1h 20m\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6643651535\nbestIteration = 761\n\nShrink model to first 762 iterations.\n============ Fold 1 ============\n\n0:\tlearn: 1.0831624\ttest: 1.0831592\tbest: 1.0831592 (0)\ttotal: 245ms\tremaining: 40m 51s\n100:\tlearn: 0.7348941\ttest: 0.6976274\tbest: 0.6976274 (100)\ttotal: 38.9s\tremaining: 1h 3m 36s\n200:\tlearn: 0.7091219\ttest: 0.6726237\tbest: 0.6726237 (200)\ttotal: 1m 29s\tremaining: 1h 12m 45s\n300:\tlearn: 0.6922515\ttest: 0.6687022\tbest: 0.6687022 (300)\ttotal: 2m 23s\tremaining: 1h 16m 51s\n400:\tlearn: 0.6724169\ttest: 0.6655868\tbest: 0.6655868 (400)\ttotal: 3m 19s\tremaining: 1h 19m 27s\n500:\tlearn: 0.6510442\ttest: 0.6630204\tbest: 0.6630204 (500)\ttotal: 4m 15s\tremaining: 1h 20m 48s\n600:\tlearn: 0.6316988\ttest: 0.6614526\tbest: 0.6614526 (600)\ttotal: 5m 11s\tremaining: 1h 21m 7s\n700:\tlearn: 0.6129803\ttest: 0.6607424\tbest: 0.6606660 (687)\ttotal: 6m 6s\tremaining: 1h 21m 6s\n800:\tlearn: 0.5930475\ttest: 0.6602049\tbest: 0.6601429 (793)\ttotal: 7m 3s\tremaining: 1h 21m\n900:\tlearn: 0.5758447\ttest: 0.6597748\tbest: 0.6596636 (885)\ttotal: 7m 59s\tremaining: 1h 20m 44s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6596635705\nbestIteration = 885\n\nShrink model to first 886 iterations.\n============ Fold 2 ============\n\n0:\tlearn: 1.0831179\ttest: 1.0832346\tbest: 1.0832346 (0)\ttotal: 252ms\tremaining: 42m 1s\n100:\tlearn: 0.7276329\ttest: 0.6945366\tbest: 0.6945366 (100)\ttotal: 40.6s\tremaining: 1h 6m 18s\n200:\tlearn: 0.7020572\ttest: 0.6763344\tbest: 0.6763344 (200)\ttotal: 1m 35s\tremaining: 1h 17m 11s\n300:\tlearn: 0.6849021\ttest: 0.6734732\tbest: 0.6734732 (300)\ttotal: 2m 27s\tremaining: 1h 19m 24s\n400:\tlearn: 0.6678492\ttest: 0.6713501\tbest: 0.6713501 (400)\ttotal: 3m 21s\tremaining: 1h 20m 34s\n500:\tlearn: 0.6489240\ttest: 0.6697702\tbest: 0.6697702 (500)\ttotal: 4m 18s\tremaining: 1h 21m 32s\n600:\tlearn: 0.6286190\ttest: 0.6682808\tbest: 0.6682423 (598)\ttotal: 5m 13s\tremaining: 1h 21m 46s\n700:\tlearn: 0.6096438\ttest: 0.6678665\tbest: 0.6678645 (698)\ttotal: 6m 9s\tremaining: 1h 21m 46s\n800:\tlearn: 0.5927210\ttest: 0.6675491\tbest: 0.6673454 (766)\ttotal: 7m 5s\tremaining: 1h 21m 23s\n900:\tlearn: 0.5757733\ttest: 0.6672852\tbest: 0.6670491 (844)\ttotal: 8m\tremaining: 1h 20m 52s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6670490964\nbestIteration = 844\n\nShrink model to first 845 iterations.\n============ Fold 3 ============\n\n0:\tlearn: 1.0831514\ttest: 1.0830642\tbest: 1.0830642 (0)\ttotal: 249ms\tremaining: 41m 29s\n100:\tlearn: 0.7303778\ttest: 0.7092536\tbest: 0.7092536 (100)\ttotal: 38.9s\tremaining: 1h 3m 36s\n200:\tlearn: 0.7023113\ttest: 0.6888512\tbest: 0.6888512 (200)\ttotal: 1m 31s\tremaining: 1h 14m 10s\n300:\tlearn: 0.6853219\ttest: 0.6845243\tbest: 0.6845243 (300)\ttotal: 2m 25s\tremaining: 1h 18m 8s\n400:\tlearn: 0.6675973\ttest: 0.6826500\tbest: 0.6826052 (388)\ttotal: 3m 20s\tremaining: 1h 20m 1s\n500:\tlearn: 0.6481532\ttest: 0.6808902\tbest: 0.6807720 (498)\ttotal: 4m 15s\tremaining: 1h 20m 50s\n600:\tlearn: 0.6285784\ttest: 0.6800921\tbest: 0.6800914 (595)\ttotal: 5m 12s\tremaining: 1h 21m 20s\n700:\tlearn: 0.6083968\ttest: 0.6796023\tbest: 0.6794399 (663)\ttotal: 6m 7s\tremaining: 1h 21m 14s\n800:\tlearn: 0.5899674\ttest: 0.6789752\tbest: 0.6788943 (792)\ttotal: 7m 2s\tremaining: 1h 20m 57s\n900:\tlearn: 0.5721327\ttest: 0.6783820\tbest: 0.6783820 (900)\ttotal: 7m 59s\tremaining: 1h 20m 38s\n1000:\tlearn: 0.5544633\ttest: 0.6780686\tbest: 0.6779982 (942)\ttotal: 8m 53s\tremaining: 1h 19m 59s\n1100:\tlearn: 0.5367893\ttest: 0.6780652\tbest: 0.6776773 (1037)\ttotal: 9m 49s\tremaining: 1h 19m 28s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6776773442\nbestIteration = 1037\n\nShrink model to first 1038 iterations.\n============ Fold 4 ============\n\n0:\tlearn: 1.0831773\ttest: 1.0829793\tbest: 1.0829793 (0)\ttotal: 243ms\tremaining: 40m 32s\n100:\tlearn: 0.7273797\ttest: 0.6978671\tbest: 0.6978671 (100)\ttotal: 42.9s\tremaining: 1h 10m 1s\n200:\tlearn: 0.7017828\ttest: 0.6804129\tbest: 0.6804129 (200)\ttotal: 1m 35s\tremaining: 1h 17m 49s\n300:\tlearn: 0.6860989\ttest: 0.6769156\tbest: 0.6768962 (299)\ttotal: 2m 29s\tremaining: 1h 20m 5s\n400:\tlearn: 0.6681431\ttest: 0.6751760\tbest: 0.6751278 (397)\ttotal: 3m 24s\tremaining: 1h 21m 32s\n500:\tlearn: 0.6485897\ttest: 0.6737932\tbest: 0.6737814 (499)\ttotal: 4m 19s\tremaining: 1h 22m\n600:\tlearn: 0.6285877\ttest: 0.6730592\tbest: 0.6728910 (598)\ttotal: 5m 15s\tremaining: 1h 22m 18s\n700:\tlearn: 0.6097368\ttest: 0.6721781\tbest: 0.6721426 (699)\ttotal: 6m 10s\tremaining: 1h 21m 53s\n800:\tlearn: 0.5926144\ttest: 0.6721474\tbest: 0.6719096 (776)\ttotal: 7m 6s\tremaining: 1h 21m 34s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6719096303\nbestIteration = 776\n\nShrink model to first 777 iterations.\n============ Fold 5 ============\n\n0:\tlearn: 1.0832186\ttest: 1.0833499\tbest: 1.0833499 (0)\ttotal: 243ms\tremaining: 40m 30s\n100:\tlearn: 0.7249238\ttest: 0.7056807\tbest: 0.7056807 (100)\ttotal: 39.7s\tremaining: 1h 4m 51s\n200:\tlearn: 0.6995494\ttest: 0.6906425\tbest: 0.6906425 (200)\ttotal: 1m 31s\tremaining: 1h 14m 12s\n300:\tlearn: 0.6821473\ttest: 0.6874748\tbest: 0.6874748 (300)\ttotal: 2m 24s\tremaining: 1h 17m 44s\n400:\tlearn: 0.6642446\ttest: 0.6860589\tbest: 0.6860589 (400)\ttotal: 3m 18s\tremaining: 1h 19m 19s\n500:\tlearn: 0.6449000\ttest: 0.6855384\tbest: 0.6855013 (499)\ttotal: 4m 14s\tremaining: 1h 20m 16s\n600:\tlearn: 0.6241052\ttest: 0.6848923\tbest: 0.6848844 (589)\ttotal: 5m 9s\tremaining: 1h 20m 40s\n700:\tlearn: 0.6055032\ttest: 0.6847242\tbest: 0.6844358 (666)\ttotal: 6m 4s\tremaining: 1h 20m 37s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6844358101\nbestIteration = 666\n\nShrink model to first 667 iterations.\n============ Fold 6 ============\n\n0:\tlearn: 1.0831277\ttest: 1.0830831\tbest: 1.0830831 (0)\ttotal: 249ms\tremaining: 41m 28s\n100:\tlearn: 0.7299032\ttest: 0.7012839\tbest: 0.7012839 (100)\ttotal: 41.7s\tremaining: 1h 8m 3s\n200:\tlearn: 0.7043960\ttest: 0.6834182\tbest: 0.6833724 (188)\ttotal: 1m 34s\tremaining: 1h 16m 59s\n300:\tlearn: 0.6855325\ttest: 0.6793321\tbest: 0.6793321 (300)\ttotal: 2m 27s\tremaining: 1h 19m 28s\n400:\tlearn: 0.6663620\ttest: 0.6766051\tbest: 0.6766051 (400)\ttotal: 3m 22s\tremaining: 1h 20m 55s\n500:\tlearn: 0.6459442\ttest: 0.6751095\tbest: 0.6750316 (451)\ttotal: 4m 17s\tremaining: 1h 21m 29s\n600:\tlearn: 0.6271073\ttest: 0.6742075\tbest: 0.6741912 (599)\ttotal: 5m 13s\tremaining: 1h 21m 41s\n700:\tlearn: 0.6080896\ttest: 0.6741347\tbest: 0.6738342 (628)\ttotal: 6m 9s\tremaining: 1h 21m 41s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6738341764\nbestIteration = 628\n\nShrink model to first 629 iterations.\n============ Fold 7 ============\n\n0:\tlearn: 1.0832149\ttest: 1.0831962\tbest: 1.0831962 (0)\ttotal: 241ms\tremaining: 40m 8s\n100:\tlearn: 0.7276994\ttest: 0.7009006\tbest: 0.7009006 (100)\ttotal: 40.2s\tremaining: 1h 5m 35s\n200:\tlearn: 0.7030936\ttest: 0.6848146\tbest: 0.6847949 (198)\ttotal: 1m 33s\tremaining: 1h 16m\n300:\tlearn: 0.6856515\ttest: 0.6815006\tbest: 0.6815006 (300)\ttotal: 2m 27s\tremaining: 1h 19m 5s\n400:\tlearn: 0.6681527\ttest: 0.6796503\tbest: 0.6795603 (384)\ttotal: 3m 22s\tremaining: 1h 20m 38s\n500:\tlearn: 0.6503799\ttest: 0.6787021\tbest: 0.6785460 (493)\ttotal: 4m 17s\tremaining: 1h 21m 19s\n600:\tlearn: 0.6319088\ttest: 0.6780125\tbest: 0.6779053 (597)\ttotal: 5m 12s\tremaining: 1h 21m 34s\n700:\tlearn: 0.6122103\ttest: 0.6774097\tbest: 0.6773104 (698)\ttotal: 6m 8s\tremaining: 1h 21m 28s\n800:\tlearn: 0.5939343\ttest: 0.6764882\tbest: 0.6764710 (799)\ttotal: 7m 5s\tremaining: 1h 21m 23s\n900:\tlearn: 0.5762685\ttest: 0.6764979\tbest: 0.6762442 (823)\ttotal: 8m 1s\tremaining: 1h 21m 3s\n1000:\tlearn: 0.5586911\ttest: 0.6759459\tbest: 0.6759226 (999)\ttotal: 8m 57s\tremaining: 1h 20m 29s\n1100:\tlearn: 0.5415433\ttest: 0.6763338\tbest: 0.6757223 (1032)\ttotal: 9m 53s\tremaining: 1h 19m 53s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6757223098\nbestIteration = 1032\n\nShrink model to first 1033 iterations.\n============ Fold 8 ============\n\n0:\tlearn: 1.0831574\ttest: 1.0832749\tbest: 1.0832749 (0)\ttotal: 244ms\tremaining: 40m 44s\n100:\tlearn: 0.7315047\ttest: 0.7031190\tbest: 0.7031190 (100)\ttotal: 36s\tremaining: 58m 48s\n200:\tlearn: 0.7056228\ttest: 0.6837846\tbest: 0.6837846 (200)\ttotal: 1m 27s\tremaining: 1h 11m 24s\n300:\tlearn: 0.6886137\ttest: 0.6786870\tbest: 0.6786859 (299)\ttotal: 2m 21s\tremaining: 1h 15m 51s\n400:\tlearn: 0.6690231\ttest: 0.6763612\tbest: 0.6762533 (394)\ttotal: 3m 15s\tremaining: 1h 18m 8s\n500:\tlearn: 0.6483094\ttest: 0.6741831\tbest: 0.6741430 (497)\ttotal: 4m 10s\tremaining: 1h 19m 9s\n600:\tlearn: 0.6292264\ttest: 0.6731320\tbest: 0.6731320 (600)\ttotal: 5m 6s\tremaining: 1h 19m 51s\n700:\tlearn: 0.6111467\ttest: 0.6726734\tbest: 0.6726604 (662)\ttotal: 6m 1s\tremaining: 1h 19m 56s\n800:\tlearn: 0.5925022\ttest: 0.6727895\tbest: 0.6724140 (727)\ttotal: 6m 55s\tremaining: 1h 19m 34s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.672413971\nbestIteration = 727\n\nShrink model to first 728 iterations.\n============ Fold 9 ============\n\n0:\tlearn: 1.0832009\ttest: 1.0827439\tbest: 1.0827439 (0)\ttotal: 241ms\tremaining: 40m 13s\n100:\tlearn: 0.7324903\ttest: 0.7048082\tbest: 0.7048082 (100)\ttotal: 35.7s\tremaining: 58m 16s\n200:\tlearn: 0.7065402\ttest: 0.6859564\tbest: 0.6859419 (198)\ttotal: 1m 24s\tremaining: 1h 8m 35s\n300:\tlearn: 0.6876098\ttest: 0.6816683\tbest: 0.6816683 (300)\ttotal: 2m 18s\tremaining: 1h 14m 13s\n400:\tlearn: 0.6702645\ttest: 0.6800155\tbest: 0.6800155 (400)\ttotal: 3m 13s\tremaining: 1h 17m 18s\n500:\tlearn: 0.6496183\ttest: 0.6780545\tbest: 0.6779703 (494)\ttotal: 4m 7s\tremaining: 1h 18m 21s\n600:\tlearn: 0.6291321\ttest: 0.6770233\tbest: 0.6769609 (596)\ttotal: 5m 3s\tremaining: 1h 19m 4s\n700:\tlearn: 0.6095260\ttest: 0.6765122\tbest: 0.6765086 (674)\ttotal: 5m 58s\tremaining: 1h 19m 18s\n800:\tlearn: 0.5920527\ttest: 0.6764972\tbest: 0.6763723 (712)\ttotal: 6m 53s\tremaining: 1h 19m 5s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6763722857\nbestIteration = 712\n\nShrink model to first 713 iterations.\nLog Loss Score: 0.67234\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LGBM Train\n+ 하이퍼파라미터 튜닝은 optuna를 사용했음","metadata":{}},{"cell_type":"code","source":"train, test = load_dataset()\nX = train.drop(\"credit\", axis=1)\ny = train[\"credit\"]\nX_test = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:38:25.617672Z","iopub.execute_input":"2021-05-24T04:38:25.618271Z","iopub.status.idle":"2021-05-24T04:38:25.929996Z","shell.execute_reply.started":"2021-05-24T04:38:25.618225Z","shell.execute_reply":"2021-05-24T04:38:25.929038Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"lgb_params = {\n    \"reg_alpha\": 5.998770177220496e-05,\n    \"reg_lambda\": 0.07127674208132959,\n    \"max_depth\": 18,\n    \"num_leaves\": 125,\n    \"colsample_bytree\": 0.4241631237880101,\n    \"subsample\": 0.8876057928391585,\n    \"subsample_freq\": 5,\n    \"min_child_samples\": 5,\n    \"max_bin\": 449,\n    \"random_state\": 42,\n    \"boosting_type\": \"gbdt\",\n    \"learning_rate\": 0.05,\n    \"n_estimators\": 10000,\n    \"objective\": \"multiclass\",\n    \"metric\": \"multi_logloss\",\n}\nlgbm_oof, lgbm_preds = stratified_kfold_lgbm(lgb_params, 10, X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:38:25.932517Z","iopub.execute_input":"2021-05-24T04:38:25.932882Z","iopub.status.idle":"2021-05-24T04:41:27.262827Z","shell.execute_reply.started":"2021-05-24T04:38:25.932843Z","shell.execute_reply":"2021-05-24T04:41:27.261743Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"============ Fold 0 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.558297\tvalid_1's multi_logloss: 0.710048\n[200]\ttraining's multi_logloss: 0.444307\tvalid_1's multi_logloss: 0.68103\n[300]\ttraining's multi_logloss: 0.370455\tvalid_1's multi_logloss: 0.675189\n[400]\ttraining's multi_logloss: 0.314399\tvalid_1's multi_logloss: 0.680516\nEarly stopping, best iteration is:\n[311]\ttraining's multi_logloss: 0.364184\tvalid_1's multi_logloss: 0.674869\nTraining until validation scores don't improve for 100 rounds\n[400]\ttraining's multi_logloss: 0.358725\tvalid_1's multi_logloss: 0.674764\n[500]\ttraining's multi_logloss: 0.352808\tvalid_1's multi_logloss: 0.67485\nEarly stopping, best iteration is:\n[407]\ttraining's multi_logloss: 0.358272\tvalid_1's multi_logloss: 0.674724\n============ Fold 1 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.557051\tvalid_1's multi_logloss: 0.715119\n[200]\ttraining's multi_logloss: 0.442376\tvalid_1's multi_logloss: 0.688082\n[300]\ttraining's multi_logloss: 0.368424\tvalid_1's multi_logloss: 0.684491\nEarly stopping, best iteration is:\n[286]\ttraining's multi_logloss: 0.37776\tvalid_1's multi_logloss: 0.683915\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.376758\tvalid_1's multi_logloss: 0.683944\nEarly stopping, best iteration is:\n[293]\ttraining's multi_logloss: 0.37729\tvalid_1's multi_logloss: 0.683882\n============ Fold 2 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.559079\tvalid_1's multi_logloss: 0.709535\n[200]\ttraining's multi_logloss: 0.444743\tvalid_1's multi_logloss: 0.685369\n[300]\ttraining's multi_logloss: 0.370999\tvalid_1's multi_logloss: 0.68545\nEarly stopping, best iteration is:\n[269]\ttraining's multi_logloss: 0.391609\tvalid_1's multi_logloss: 0.683632\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.389411\tvalid_1's multi_logloss: 0.683654\n[400]\ttraining's multi_logloss: 0.382514\tvalid_1's multi_logloss: 0.683673\nEarly stopping, best iteration is:\n[368]\ttraining's multi_logloss: 0.384574\tvalid_1's multi_logloss: 0.68357\n============ Fold 3 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.55667\tvalid_1's multi_logloss: 0.72357\n[200]\ttraining's multi_logloss: 0.443354\tvalid_1's multi_logloss: 0.701939\n[300]\ttraining's multi_logloss: 0.369061\tvalid_1's multi_logloss: 0.701999\nEarly stopping, best iteration is:\n[221]\ttraining's multi_logloss: 0.425229\tvalid_1's multi_logloss: 0.699529\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.418732\tvalid_1's multi_logloss: 0.699265\n[400]\ttraining's multi_logloss: 0.410996\tvalid_1's multi_logloss: 0.698926\n[500]\ttraining's multi_logloss: 0.403574\tvalid_1's multi_logloss: 0.69874\n[600]\ttraining's multi_logloss: 0.395812\tvalid_1's multi_logloss: 0.698748\nEarly stopping, best iteration is:\n[508]\ttraining's multi_logloss: 0.403002\tvalid_1's multi_logloss: 0.698689\n============ Fold 4 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.559749\tvalid_1's multi_logloss: 0.709381\n[200]\ttraining's multi_logloss: 0.444327\tvalid_1's multi_logloss: 0.681738\n[300]\ttraining's multi_logloss: 0.369887\tvalid_1's multi_logloss: 0.677157\nEarly stopping, best iteration is:\n[278]\ttraining's multi_logloss: 0.384456\tvalid_1's multi_logloss: 0.676522\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.382884\tvalid_1's multi_logloss: 0.67655\nEarly stopping, best iteration is:\n[291]\ttraining's multi_logloss: 0.383463\tvalid_1's multi_logloss: 0.67649\n============ Fold 5 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.556635\tvalid_1's multi_logloss: 0.728675\n[200]\ttraining's multi_logloss: 0.440301\tvalid_1's multi_logloss: 0.705799\n[300]\ttraining's multi_logloss: 0.365431\tvalid_1's multi_logloss: 0.708614\nEarly stopping, best iteration is:\n[249]\ttraining's multi_logloss: 0.399944\tvalid_1's multi_logloss: 0.703978\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.396066\tvalid_1's multi_logloss: 0.704059\nEarly stopping, best iteration is:\n[256]\ttraining's multi_logloss: 0.399432\tvalid_1's multi_logloss: 0.703956\n============ Fold 6 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.558638\tvalid_1's multi_logloss: 0.718429\n[200]\ttraining's multi_logloss: 0.443756\tvalid_1's multi_logloss: 0.692409\n[300]\ttraining's multi_logloss: 0.370155\tvalid_1's multi_logloss: 0.687175\nEarly stopping, best iteration is:\n[287]\ttraining's multi_logloss: 0.379327\tvalid_1's multi_logloss: 0.686592\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.37843\tvalid_1's multi_logloss: 0.686592\n[400]\ttraining's multi_logloss: 0.371816\tvalid_1's multi_logloss: 0.686755\nEarly stopping, best iteration is:\n[320]\ttraining's multi_logloss: 0.37714\tvalid_1's multi_logloss: 0.686518\n============ Fold 7 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.558191\tvalid_1's multi_logloss: 0.716947\n[200]\ttraining's multi_logloss: 0.444287\tvalid_1's multi_logloss: 0.690699\n[300]\ttraining's multi_logloss: 0.368962\tvalid_1's multi_logloss: 0.689638\nEarly stopping, best iteration is:\n[269]\ttraining's multi_logloss: 0.389597\tvalid_1's multi_logloss: 0.687518\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.387454\tvalid_1's multi_logloss: 0.687559\nEarly stopping, best iteration is:\n[273]\ttraining's multi_logloss: 0.389322\tvalid_1's multi_logloss: 0.687514\n============ Fold 8 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.556743\tvalid_1's multi_logloss: 0.721669\n[200]\ttraining's multi_logloss: 0.441304\tvalid_1's multi_logloss: 0.700372\n[300]\ttraining's multi_logloss: 0.367715\tvalid_1's multi_logloss: 0.699101\nEarly stopping, best iteration is:\n[266]\ttraining's multi_logloss: 0.390431\tvalid_1's multi_logloss: 0.697598\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.388073\tvalid_1's multi_logloss: 0.697634\nEarly stopping, best iteration is:\n[267]\ttraining's multi_logloss: 0.390377\tvalid_1's multi_logloss: 0.697596\n============ Fold 9 ============\n\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's multi_logloss: 0.556094\tvalid_1's multi_logloss: 0.714247\n[200]\ttraining's multi_logloss: 0.442896\tvalid_1's multi_logloss: 0.692599\n[300]\ttraining's multi_logloss: 0.368471\tvalid_1's multi_logloss: 0.691702\nEarly stopping, best iteration is:\n[262]\ttraining's multi_logloss: 0.394171\tvalid_1's multi_logloss: 0.690039\nTraining until validation scores don't improve for 100 rounds\n[300]\ttraining's multi_logloss: 0.391432\tvalid_1's multi_logloss: 0.690078\nEarly stopping, best iteration is:\n[283]\ttraining's multi_logloss: 0.392614\tvalid_1's multi_logloss: 0.690004\nLog Loss Score: 0.68829\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# XGB Train\n+ 하이퍼파라미터 튜닝을 optuna를 사용했음","metadata":{}},{"cell_type":"code","source":"xgb_params = {\n    \"eta\": 0.023839252347297356,\n    \"reg_alpha\": 6.99554614267605e-06,\n    \"reg_lambda\": 0.010419988953061583,\n    \"max_depth\": 15,\n    \"max_leaves\": 159,\n    \"colsample_bytree\": 0.4515469593932409,\n    \"subsample\": 0.7732694309118915,\n    \"min_child_weight\": 5,\n    \"gamma\": 0.6847131315687576,\n    \"random_state\": 42,\n    \"n_estimators\": 10000,\n    \"objective\": \"multi:softmax\",\n    \"eval_metric\": \"mlogloss\",\n}\nxgb_oof, xgb_preds = stratified_kfold_xgb(xgb_params, 10, X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:41:27.264591Z","iopub.execute_input":"2021-05-24T04:41:27.264935Z","iopub.status.idle":"2021-05-24T04:51:02.999834Z","shell.execute_reply.started":"2021-05-24T04:41:27.264897Z","shell.execute_reply":"2021-05-24T04:51:02.999034Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"============ Fold 0 ============\n\n[0]\tvalidation_0-mlogloss:1.08671\tvalidation_1-mlogloss:1.08830\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[100]\tvalidation_0-mlogloss:0.60860\tvalidation_1-mlogloss:0.73678\n[200]\tvalidation_0-mlogloss:0.49016\tvalidation_1-mlogloss:0.69367\n[300]\tvalidation_0-mlogloss:0.42604\tvalidation_1-mlogloss:0.68027\n[400]\tvalidation_0-mlogloss:0.38186\tvalidation_1-mlogloss:0.67709\n[497]\tvalidation_0-mlogloss:0.34856\tvalidation_1-mlogloss:0.67779\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n","output_type":"stream"},{"name":"stdout","text":"============ Fold 1 ============\n\n[0]\tvalidation_0-mlogloss:1.08610\tvalidation_1-mlogloss:1.08795\n[100]\tvalidation_0-mlogloss:0.61238\tvalidation_1-mlogloss:0.74311\n[200]\tvalidation_0-mlogloss:0.49265\tvalidation_1-mlogloss:0.70016\n[300]\tvalidation_0-mlogloss:0.42657\tvalidation_1-mlogloss:0.68767\n[400]\tvalidation_0-mlogloss:0.38175\tvalidation_1-mlogloss:0.68516\n[500]\tvalidation_0-mlogloss:0.34796\tvalidation_1-mlogloss:0.68706\n[518]\tvalidation_0-mlogloss:0.34252\tvalidation_1-mlogloss:0.68770\n============ Fold 2 ============\n\n[0]\tvalidation_0-mlogloss:1.08659\tvalidation_1-mlogloss:1.08834\n[100]\tvalidation_0-mlogloss:0.61568\tvalidation_1-mlogloss:0.74170\n[200]\tvalidation_0-mlogloss:0.49399\tvalidation_1-mlogloss:0.69688\n[300]\tvalidation_0-mlogloss:0.42702\tvalidation_1-mlogloss:0.68586\n[400]\tvalidation_0-mlogloss:0.38326\tvalidation_1-mlogloss:0.68467\n[466]\tvalidation_0-mlogloss:0.35951\tvalidation_1-mlogloss:0.68507\n============ Fold 3 ============\n\n[0]\tvalidation_0-mlogloss:1.08586\tvalidation_1-mlogloss:1.08814\n[100]\tvalidation_0-mlogloss:0.61298\tvalidation_1-mlogloss:0.74823\n[200]\tvalidation_0-mlogloss:0.49382\tvalidation_1-mlogloss:0.70872\n[300]\tvalidation_0-mlogloss:0.42853\tvalidation_1-mlogloss:0.70048\n[400]\tvalidation_0-mlogloss:0.38378\tvalidation_1-mlogloss:0.70023\n[456]\tvalidation_0-mlogloss:0.36341\tvalidation_1-mlogloss:0.70123\n============ Fold 4 ============\n\n[0]\tvalidation_0-mlogloss:1.08610\tvalidation_1-mlogloss:1.08800\n[100]\tvalidation_0-mlogloss:0.61311\tvalidation_1-mlogloss:0.73966\n[200]\tvalidation_0-mlogloss:0.49023\tvalidation_1-mlogloss:0.69509\n[300]\tvalidation_0-mlogloss:0.42441\tvalidation_1-mlogloss:0.68346\n[400]\tvalidation_0-mlogloss:0.38015\tvalidation_1-mlogloss:0.68037\n[500]\tvalidation_0-mlogloss:0.34675\tvalidation_1-mlogloss:0.68140\n[514]\tvalidation_0-mlogloss:0.34254\tvalidation_1-mlogloss:0.68174\n============ Fold 5 ============\n\n[0]\tvalidation_0-mlogloss:1.08681\tvalidation_1-mlogloss:1.08855\n[100]\tvalidation_0-mlogloss:0.61195\tvalidation_1-mlogloss:0.75309\n[200]\tvalidation_0-mlogloss:0.49143\tvalidation_1-mlogloss:0.71664\n[300]\tvalidation_0-mlogloss:0.42520\tvalidation_1-mlogloss:0.71000\n[400]\tvalidation_0-mlogloss:0.37887\tvalidation_1-mlogloss:0.71086\n[432]\tvalidation_0-mlogloss:0.36822\tvalidation_1-mlogloss:0.71160\n============ Fold 6 ============\n\n[0]\tvalidation_0-mlogloss:1.08592\tvalidation_1-mlogloss:1.08840\n[100]\tvalidation_0-mlogloss:0.61311\tvalidation_1-mlogloss:0.74551\n[200]\tvalidation_0-mlogloss:0.49330\tvalidation_1-mlogloss:0.70115\n[300]\tvalidation_0-mlogloss:0.42652\tvalidation_1-mlogloss:0.68875\n[400]\tvalidation_0-mlogloss:0.38145\tvalidation_1-mlogloss:0.68545\n[500]\tvalidation_0-mlogloss:0.34812\tvalidation_1-mlogloss:0.68636\n[511]\tvalidation_0-mlogloss:0.34473\tvalidation_1-mlogloss:0.68679\n============ Fold 7 ============\n\n[0]\tvalidation_0-mlogloss:1.08657\tvalidation_1-mlogloss:1.08863\n[100]\tvalidation_0-mlogloss:0.61477\tvalidation_1-mlogloss:0.74560\n[200]\tvalidation_0-mlogloss:0.49598\tvalidation_1-mlogloss:0.70359\n[300]\tvalidation_0-mlogloss:0.42721\tvalidation_1-mlogloss:0.69131\n[400]\tvalidation_0-mlogloss:0.38060\tvalidation_1-mlogloss:0.68940\n[500]\tvalidation_0-mlogloss:0.34677\tvalidation_1-mlogloss:0.69192\n============ Fold 8 ============\n\n[0]\tvalidation_0-mlogloss:1.08615\tvalidation_1-mlogloss:1.08787\n[100]\tvalidation_0-mlogloss:0.61230\tvalidation_1-mlogloss:0.74758\n[200]\tvalidation_0-mlogloss:0.49440\tvalidation_1-mlogloss:0.70954\n[300]\tvalidation_0-mlogloss:0.42756\tvalidation_1-mlogloss:0.70042\n[400]\tvalidation_0-mlogloss:0.38250\tvalidation_1-mlogloss:0.69977\n[489]\tvalidation_0-mlogloss:0.35041\tvalidation_1-mlogloss:0.70161\n============ Fold 9 ============\n\n[0]\tvalidation_0-mlogloss:1.08661\tvalidation_1-mlogloss:1.08835\n[100]\tvalidation_0-mlogloss:0.61319\tvalidation_1-mlogloss:0.74565\n[200]\tvalidation_0-mlogloss:0.49023\tvalidation_1-mlogloss:0.70382\n[300]\tvalidation_0-mlogloss:0.42508\tvalidation_1-mlogloss:0.69486\n[400]\tvalidation_0-mlogloss:0.38028\tvalidation_1-mlogloss:0.69438\n[442]\tvalidation_0-mlogloss:0.36562\tvalidation_1-mlogloss:0.69537\nLog Loss Score: 0.69034\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RandomForest Train","metadata":{}},{"cell_type":"code","source":"rf_params = {\n        \"criterion\": \"gini\",\n        \"n_estimators\": 300,\n        \"min_samples_split\": 10,\n        \"min_samples_leaf\": 2,\n        \"max_features\": \"auto\",\n        \"oob_score\": True,\n        \"random_state\": 42,\n        \"n_jobs\": -1,\n    }\nrf_oof, rf_preds = stratified_kfold_rf(rf_params, 10, X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:51:03.001315Z","iopub.execute_input":"2021-05-24T04:51:03.001643Z","iopub.status.idle":"2021-05-24T04:52:28.223881Z","shell.execute_reply.started":"2021-05-24T04:51:03.001606Z","shell.execute_reply":"2021-05-24T04:52:28.223049Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"============ Fold 0 ============\n\nLog Loss Score: 0.68083\n============ Fold 1 ============\n\nLog Loss Score: 0.68767\n============ Fold 2 ============\n\nLog Loss Score: 0.68559\n============ Fold 3 ============\n\nLog Loss Score: 0.69447\n============ Fold 4 ============\n\nLog Loss Score: 0.68046\n============ Fold 5 ============\n\nLog Loss Score: 0.70892\n============ Fold 6 ============\n\nLog Loss Score: 0.69037\n============ Fold 7 ============\n\nLog Loss Score: 0.69148\n============ Fold 8 ============\n\nLog Loss Score: 0.69465\n============ Fold 9 ============\n\nLog Loss Score: 0.69926\nLog Loss Score: 0.69137\n","output_type":"stream"}]},{"cell_type":"code","source":"train, test = load_dataset()\ntrain_x = train.drop(\"credit\", axis = 1)\ntrain_y = train['credit'].values","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:52:28.225231Z","iopub.execute_input":"2021-05-24T04:52:28.225718Z","iopub.status.idle":"2021-05-24T04:52:28.633099Z","shell.execute_reply.started":"2021-05-24T04:52:28.225678Z","shell.execute_reply":"2021-05-24T04:52:28.632139Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_pred = np.concatenate([cat_oof, lgbm_oof, xgb_oof, rf_oof], axis=1)\ntrain_pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:52:28.634352Z","iopub.execute_input":"2021-05-24T04:52:28.634938Z","iopub.status.idle":"2021-05-24T04:52:28.642952Z","shell.execute_reply.started":"2021-05-24T04:52:28.634896Z","shell.execute_reply":"2021-05-24T04:52:28.641787Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(26451, 12)"},"metadata":{}}]},{"cell_type":"code","source":"test_pred = np.concatenate([cat_preds, lgbm_preds, xgb_preds, rf_preds], axis=1)\ntest_pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:52:28.644375Z","iopub.execute_input":"2021-05-24T04:52:28.645045Z","iopub.status.idle":"2021-05-24T04:52:28.652160Z","shell.execute_reply.started":"2021-05-24T04:52:28.644976Z","shell.execute_reply":"2021-05-24T04:52:28.651111Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(10000, 12)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pytorch Tabular\n+ Stacking Ensemble을 사용하며 학습 진행","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:52:28.653618Z","iopub.execute_input":"2021-05-24T04:52:28.654142Z","iopub.status.idle":"2021-05-24T04:52:28.659053Z","shell.execute_reply.started":"2021-05-24T04:52:28.654109Z","shell.execute_reply":"2021-05-24T04:52:28.658271Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"n_fold = 10\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nsplits = folds.split(train_pred, train_y)\nnet_oof = np.zeros((train_pred.shape[0], 3))\nnet_preds = np.zeros((test_pred.shape[0], 3))\nfor fold, (train_idx, valid_idx) in enumerate(splits):\n    print(f\"============ Fold {fold} ============\\n\")\n    X_train, X_valid = train_pred[train_idx], train_pred[valid_idx]\n    y_train, y_valid = train_y[train_idx], train_y[valid_idx]\n    model = TabNetMultiTaskClassifier(\n            n_d=64, n_a=64, n_steps=1,\n            lambda_sparse=1e-4,\n            optimizer_fn=torch.optim.Adam,\n            optimizer_params=dict(lr=2e-2),\n            scheduler_params = {\"gamma\": 0.9, \"step_size\": 50},\n            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n            mask_type=\"entmax\", \n            device_name=device\n    )\n\n    model.fit(\n        X_train, y_train.reshape(-1,1),\n        eval_set=[(X_valid, y_valid.reshape(-1,1))],\n        max_epochs=100,\n        batch_size=1024,\n        eval_metric=[\"logloss\"],\n        virtual_batch_size=128,\n        num_workers=1,\n        drop_last=False\n    )\n    net_oof[valid_idx] = model.predict_proba(X_valid)\n    net_preds += model.predict_proba(test_pred)[0] / n_fold\nlog_score = log_loss(train_y, net_oof)\nprint(f\"Log Loss Score: {log_score:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:52:28.660315Z","iopub.execute_input":"2021-05-24T04:52:28.660957Z","iopub.status.idle":"2021-05-24T04:59:40.592964Z","shell.execute_reply.started":"2021-05-24T04:52:28.660920Z","shell.execute_reply":"2021-05-24T04:59:40.591970Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"============ Fold 0 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.75703 | val_0_logloss: 0.79897 |  0:00:01s\nepoch 1  | loss: 0.68556 | val_0_logloss: 0.77978 |  0:00:02s\nepoch 2  | loss: 0.68048 | val_0_logloss: 0.75859 |  0:00:04s\nepoch 3  | loss: 0.67863 | val_0_logloss: 0.75964 |  0:00:05s\nepoch 4  | loss: 0.68124 | val_0_logloss: 0.75431 |  0:00:06s\nepoch 5  | loss: 0.67689 | val_0_logloss: 0.73965 |  0:00:07s\nepoch 6  | loss: 0.67523 | val_0_logloss: 0.7408  |  0:00:08s\nepoch 7  | loss: 0.67533 | val_0_logloss: 0.7257  |  0:00:10s\nepoch 8  | loss: 0.67628 | val_0_logloss: 0.7194  |  0:00:11s\nepoch 9  | loss: 0.67497 | val_0_logloss: 0.70544 |  0:00:12s\nepoch 10 | loss: 0.67502 | val_0_logloss: 0.71008 |  0:00:13s\nepoch 11 | loss: 0.67271 | val_0_logloss: 0.68738 |  0:00:14s\nepoch 12 | loss: 0.67292 | val_0_logloss: 0.68595 |  0:00:15s\nepoch 13 | loss: 0.67197 | val_0_logloss: 0.68977 |  0:00:17s\nepoch 14 | loss: 0.67286 | val_0_logloss: 0.66919 |  0:00:18s\nepoch 15 | loss: 0.67186 | val_0_logloss: 0.68656 |  0:00:19s\nepoch 16 | loss: 0.67059 | val_0_logloss: 0.66627 |  0:00:20s\nepoch 17 | loss: 0.67243 | val_0_logloss: 0.67682 |  0:00:21s\nepoch 18 | loss: 0.67134 | val_0_logloss: 0.67325 |  0:00:23s\nepoch 19 | loss: 0.67091 | val_0_logloss: 0.66412 |  0:00:24s\nepoch 20 | loss: 0.66908 | val_0_logloss: 0.66527 |  0:00:25s\nepoch 21 | loss: 0.66837 | val_0_logloss: 0.6693  |  0:00:26s\nepoch 22 | loss: 0.67108 | val_0_logloss: 0.66373 |  0:00:28s\nepoch 23 | loss: 0.67065 | val_0_logloss: 0.66239 |  0:00:29s\nepoch 24 | loss: 0.66828 | val_0_logloss: 0.66305 |  0:00:30s\nepoch 25 | loss: 0.66773 | val_0_logloss: 0.66454 |  0:00:31s\nepoch 26 | loss: 0.66873 | val_0_logloss: 0.66477 |  0:00:32s\nepoch 27 | loss: 0.67005 | val_0_logloss: 0.66604 |  0:00:33s\nepoch 28 | loss: 0.67029 | val_0_logloss: 0.66195 |  0:00:35s\nepoch 29 | loss: 0.66815 | val_0_logloss: 0.66411 |  0:00:36s\nepoch 30 | loss: 0.66782 | val_0_logloss: 0.66861 |  0:00:37s\nepoch 31 | loss: 0.6677  | val_0_logloss: 0.66263 |  0:00:38s\nepoch 32 | loss: 0.66615 | val_0_logloss: 0.66735 |  0:00:39s\nepoch 33 | loss: 0.66769 | val_0_logloss: 0.66755 |  0:00:41s\nepoch 34 | loss: 0.66673 | val_0_logloss: 0.66634 |  0:00:42s\nepoch 35 | loss: 0.66852 | val_0_logloss: 0.66559 |  0:00:43s\nepoch 36 | loss: 0.66838 | val_0_logloss: 0.66103 |  0:00:44s\nepoch 37 | loss: 0.66698 | val_0_logloss: 0.66733 |  0:00:45s\nepoch 38 | loss: 0.66684 | val_0_logloss: 0.67079 |  0:00:46s\nepoch 39 | loss: 0.66516 | val_0_logloss: 0.66535 |  0:00:48s\nepoch 40 | loss: 0.66591 | val_0_logloss: 0.66634 |  0:00:49s\nepoch 41 | loss: 0.66749 | val_0_logloss: 0.66472 |  0:00:50s\nepoch 42 | loss: 0.66719 | val_0_logloss: 0.6655  |  0:00:51s\nepoch 43 | loss: 0.66643 | val_0_logloss: 0.66642 |  0:00:52s\nepoch 44 | loss: 0.66607 | val_0_logloss: 0.66885 |  0:00:54s\nepoch 45 | loss: 0.66705 | val_0_logloss: 0.66945 |  0:00:55s\nepoch 46 | loss: 0.66741 | val_0_logloss: 0.66538 |  0:00:56s\n\nEarly stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_logloss = 0.66103\nBest weights from best epoch are automatically used!\n============ Fold 1 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.75784 | val_0_logloss: 0.79563 |  0:00:01s\nepoch 1  | loss: 0.68654 | val_0_logloss: 0.79613 |  0:00:02s\nepoch 2  | loss: 0.68146 | val_0_logloss: 0.79035 |  0:00:03s\nepoch 3  | loss: 0.6774  | val_0_logloss: 0.77993 |  0:00:05s\nepoch 4  | loss: 0.67687 | val_0_logloss: 0.78368 |  0:00:06s\nepoch 5  | loss: 0.67737 | val_0_logloss: 0.76388 |  0:00:07s\nepoch 6  | loss: 0.67758 | val_0_logloss: 0.74639 |  0:00:08s\nepoch 7  | loss: 0.67524 | val_0_logloss: 0.73145 |  0:00:09s\nepoch 8  | loss: 0.6746  | val_0_logloss: 0.72927 |  0:00:10s\nepoch 9  | loss: 0.67411 | val_0_logloss: 0.73216 |  0:00:12s\nepoch 10 | loss: 0.67581 | val_0_logloss: 0.71446 |  0:00:13s\nepoch 11 | loss: 0.67485 | val_0_logloss: 0.70395 |  0:00:14s\nepoch 12 | loss: 0.67382 | val_0_logloss: 0.69355 |  0:00:15s\nepoch 13 | loss: 0.67387 | val_0_logloss: 0.68349 |  0:00:16s\nepoch 14 | loss: 0.67322 | val_0_logloss: 0.67214 |  0:00:18s\nepoch 15 | loss: 0.67596 | val_0_logloss: 0.68645 |  0:00:19s\nepoch 16 | loss: 0.67294 | val_0_logloss: 0.6731  |  0:00:20s\nepoch 17 | loss: 0.6727  | val_0_logloss: 0.67529 |  0:00:21s\nepoch 18 | loss: 0.67198 | val_0_logloss: 0.66173 |  0:00:22s\nepoch 19 | loss: 0.67147 | val_0_logloss: 0.66399 |  0:00:23s\nepoch 20 | loss: 0.67127 | val_0_logloss: 0.67185 |  0:00:24s\nepoch 21 | loss: 0.67174 | val_0_logloss: 0.66423 |  0:00:26s\nepoch 22 | loss: 0.67167 | val_0_logloss: 0.66632 |  0:00:27s\nepoch 23 | loss: 0.6719  | val_0_logloss: 0.66194 |  0:00:28s\nepoch 24 | loss: 0.67189 | val_0_logloss: 0.67195 |  0:00:29s\nepoch 25 | loss: 0.67012 | val_0_logloss: 0.66625 |  0:00:30s\nepoch 26 | loss: 0.67111 | val_0_logloss: 0.66153 |  0:00:32s\nepoch 27 | loss: 0.67057 | val_0_logloss: 0.65844 |  0:00:33s\nepoch 28 | loss: 0.66896 | val_0_logloss: 0.66221 |  0:00:34s\nepoch 29 | loss: 0.66932 | val_0_logloss: 0.65973 |  0:00:35s\nepoch 30 | loss: 0.669   | val_0_logloss: 0.66793 |  0:00:37s\nepoch 31 | loss: 0.67002 | val_0_logloss: 0.66277 |  0:00:38s\nepoch 32 | loss: 0.66833 | val_0_logloss: 0.66818 |  0:00:39s\nepoch 33 | loss: 0.66887 | val_0_logloss: 0.65916 |  0:00:40s\nepoch 34 | loss: 0.66949 | val_0_logloss: 0.65934 |  0:00:41s\nepoch 35 | loss: 0.6688  | val_0_logloss: 0.66253 |  0:00:43s\nepoch 36 | loss: 0.67042 | val_0_logloss: 0.65917 |  0:00:44s\nepoch 37 | loss: 0.66809 | val_0_logloss: 0.66578 |  0:00:45s\n\nEarly stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_logloss = 0.65844\nBest weights from best epoch are automatically used!\n============ Fold 2 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.74781 | val_0_logloss: 0.78794 |  0:00:01s\nepoch 1  | loss: 0.68262 | val_0_logloss: 0.79278 |  0:00:02s\nepoch 2  | loss: 0.68062 | val_0_logloss: 0.79872 |  0:00:03s\nepoch 3  | loss: 0.67857 | val_0_logloss: 0.77633 |  0:00:04s\nepoch 4  | loss: 0.67567 | val_0_logloss: 0.75646 |  0:00:06s\nepoch 5  | loss: 0.67693 | val_0_logloss: 0.75099 |  0:00:07s\nepoch 6  | loss: 0.67673 | val_0_logloss: 0.73935 |  0:00:08s\nepoch 7  | loss: 0.67494 | val_0_logloss: 0.72426 |  0:00:09s\nepoch 8  | loss: 0.67348 | val_0_logloss: 0.70824 |  0:00:10s\nepoch 9  | loss: 0.67334 | val_0_logloss: 0.70252 |  0:00:11s\nepoch 10 | loss: 0.67364 | val_0_logloss: 0.69631 |  0:00:13s\nepoch 11 | loss: 0.67363 | val_0_logloss: 0.69507 |  0:00:14s\nepoch 12 | loss: 0.67425 | val_0_logloss: 0.6934  |  0:00:15s\nepoch 13 | loss: 0.67289 | val_0_logloss: 0.67599 |  0:00:16s\nepoch 14 | loss: 0.67264 | val_0_logloss: 0.67489 |  0:00:17s\nepoch 15 | loss: 0.67156 | val_0_logloss: 0.67268 |  0:00:19s\nepoch 16 | loss: 0.67047 | val_0_logloss: 0.67257 |  0:00:20s\nepoch 17 | loss: 0.67152 | val_0_logloss: 0.67261 |  0:00:21s\nepoch 18 | loss: 0.67153 | val_0_logloss: 0.66545 |  0:00:22s\nepoch 19 | loss: 0.67131 | val_0_logloss: 0.66737 |  0:00:24s\nepoch 20 | loss: 0.67075 | val_0_logloss: 0.66481 |  0:00:25s\nepoch 21 | loss: 0.6718  | val_0_logloss: 0.66641 |  0:00:26s\nepoch 22 | loss: 0.67053 | val_0_logloss: 0.66881 |  0:00:28s\nepoch 23 | loss: 0.66932 | val_0_logloss: 0.66834 |  0:00:29s\nepoch 24 | loss: 0.67098 | val_0_logloss: 0.66373 |  0:00:30s\nepoch 25 | loss: 0.67009 | val_0_logloss: 0.66402 |  0:00:31s\nepoch 26 | loss: 0.66896 | val_0_logloss: 0.67017 |  0:00:32s\nepoch 27 | loss: 0.6692  | val_0_logloss: 0.6662  |  0:00:33s\nepoch 28 | loss: 0.66822 | val_0_logloss: 0.66437 |  0:00:35s\nepoch 29 | loss: 0.66858 | val_0_logloss: 0.66511 |  0:00:36s\nepoch 30 | loss: 0.66701 | val_0_logloss: 0.6648  |  0:00:37s\nepoch 31 | loss: 0.667   | val_0_logloss: 0.6634  |  0:00:38s\nepoch 32 | loss: 0.6675  | val_0_logloss: 0.66951 |  0:00:39s\nepoch 33 | loss: 0.66666 | val_0_logloss: 0.66888 |  0:00:41s\nepoch 34 | loss: 0.66749 | val_0_logloss: 0.66479 |  0:00:42s\nepoch 35 | loss: 0.66866 | val_0_logloss: 0.66461 |  0:00:43s\nepoch 36 | loss: 0.66735 | val_0_logloss: 0.66952 |  0:00:44s\nepoch 37 | loss: 0.66711 | val_0_logloss: 0.66772 |  0:00:45s\nepoch 38 | loss: 0.66685 | val_0_logloss: 0.66659 |  0:00:46s\nepoch 39 | loss: 0.66639 | val_0_logloss: 0.67095 |  0:00:48s\nepoch 40 | loss: 0.66655 | val_0_logloss: 0.66907 |  0:00:49s\nepoch 41 | loss: 0.66726 | val_0_logloss: 0.66578 |  0:00:50s\n\nEarly stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_logloss = 0.6634\nBest weights from best epoch are automatically used!\n============ Fold 3 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.76004 | val_0_logloss: 0.82903 |  0:00:01s\nepoch 1  | loss: 0.68144 | val_0_logloss: 0.797   |  0:00:02s\nepoch 2  | loss: 0.67917 | val_0_logloss: 0.79824 |  0:00:03s\nepoch 3  | loss: 0.67516 | val_0_logloss: 0.76322 |  0:00:05s\nepoch 4  | loss: 0.67364 | val_0_logloss: 0.75428 |  0:00:07s\nepoch 5  | loss: 0.6739  | val_0_logloss: 0.74777 |  0:00:08s\nepoch 6  | loss: 0.67311 | val_0_logloss: 0.73303 |  0:00:09s\nepoch 7  | loss: 0.67155 | val_0_logloss: 0.73166 |  0:00:10s\nepoch 8  | loss: 0.67173 | val_0_logloss: 0.72134 |  0:00:12s\nepoch 9  | loss: 0.67069 | val_0_logloss: 0.7215  |  0:00:13s\nepoch 10 | loss: 0.66975 | val_0_logloss: 0.70651 |  0:00:14s\nepoch 11 | loss: 0.66918 | val_0_logloss: 0.70604 |  0:00:16s\nepoch 12 | loss: 0.67043 | val_0_logloss: 0.70369 |  0:00:17s\nepoch 13 | loss: 0.66996 | val_0_logloss: 0.69185 |  0:00:19s\nepoch 14 | loss: 0.66995 | val_0_logloss: 0.69301 |  0:00:20s\nepoch 15 | loss: 0.66881 | val_0_logloss: 0.6879  |  0:00:22s\nepoch 16 | loss: 0.66859 | val_0_logloss: 0.68336 |  0:00:23s\nepoch 17 | loss: 0.66869 | val_0_logloss: 0.68212 |  0:00:24s\nepoch 18 | loss: 0.66957 | val_0_logloss: 0.68537 |  0:00:26s\nepoch 19 | loss: 0.67302 | val_0_logloss: 0.68697 |  0:00:27s\nepoch 20 | loss: 0.66908 | val_0_logloss: 0.67828 |  0:00:29s\nepoch 21 | loss: 0.66984 | val_0_logloss: 0.68131 |  0:00:30s\nepoch 22 | loss: 0.66943 | val_0_logloss: 0.68024 |  0:00:32s\nepoch 23 | loss: 0.66766 | val_0_logloss: 0.68055 |  0:00:33s\nepoch 24 | loss: 0.66601 | val_0_logloss: 0.6801  |  0:00:35s\nepoch 25 | loss: 0.66552 | val_0_logloss: 0.68332 |  0:00:36s\nepoch 26 | loss: 0.66743 | val_0_logloss: 0.68132 |  0:00:38s\nepoch 27 | loss: 0.66574 | val_0_logloss: 0.67992 |  0:00:39s\nepoch 28 | loss: 0.66848 | val_0_logloss: 0.6809  |  0:00:41s\nepoch 29 | loss: 0.66512 | val_0_logloss: 0.68137 |  0:00:42s\nepoch 30 | loss: 0.66375 | val_0_logloss: 0.6841  |  0:00:43s\n\nEarly stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_logloss = 0.67828\nBest weights from best epoch are automatically used!\n============ Fold 4 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.76234 | val_0_logloss: 0.80589 |  0:00:00s\nepoch 1  | loss: 0.68486 | val_0_logloss: 0.7979  |  0:00:01s\nepoch 2  | loss: 0.68019 | val_0_logloss: 0.77595 |  0:00:02s\nepoch 3  | loss: 0.67882 | val_0_logloss: 0.75502 |  0:00:03s\nepoch 4  | loss: 0.67733 | val_0_logloss: 0.74126 |  0:00:04s\nepoch 5  | loss: 0.68081 | val_0_logloss: 0.73469 |  0:00:05s\nepoch 6  | loss: 0.67926 | val_0_logloss: 0.72587 |  0:00:06s\nepoch 7  | loss: 0.67522 | val_0_logloss: 0.71864 |  0:00:07s\nepoch 8  | loss: 0.67376 | val_0_logloss: 0.71624 |  0:00:08s\nepoch 9  | loss: 0.67416 | val_0_logloss: 0.70773 |  0:00:09s\nepoch 10 | loss: 0.67364 | val_0_logloss: 0.7036  |  0:00:10s\nepoch 11 | loss: 0.67461 | val_0_logloss: 0.70172 |  0:00:11s\nepoch 12 | loss: 0.67316 | val_0_logloss: 0.69284 |  0:00:12s\nepoch 13 | loss: 0.67133 | val_0_logloss: 0.68339 |  0:00:13s\nepoch 14 | loss: 0.67212 | val_0_logloss: 0.68501 |  0:00:14s\nepoch 15 | loss: 0.6718  | val_0_logloss: 0.67362 |  0:00:15s\nepoch 16 | loss: 0.67025 | val_0_logloss: 0.67421 |  0:00:16s\nepoch 17 | loss: 0.67045 | val_0_logloss: 0.67373 |  0:00:17s\nepoch 18 | loss: 0.67302 | val_0_logloss: 0.68186 |  0:00:18s\nepoch 19 | loss: 0.67067 | val_0_logloss: 0.67015 |  0:00:18s\nepoch 20 | loss: 0.67015 | val_0_logloss: 0.67261 |  0:00:19s\nepoch 21 | loss: 0.67026 | val_0_logloss: 0.67038 |  0:00:20s\nepoch 22 | loss: 0.66966 | val_0_logloss: 0.67219 |  0:00:21s\nepoch 23 | loss: 0.66897 | val_0_logloss: 0.67258 |  0:00:22s\nepoch 24 | loss: 0.67071 | val_0_logloss: 0.66794 |  0:00:23s\nepoch 25 | loss: 0.67005 | val_0_logloss: 0.67428 |  0:00:24s\nepoch 26 | loss: 0.66926 | val_0_logloss: 0.66679 |  0:00:25s\nepoch 27 | loss: 0.66841 | val_0_logloss: 0.66662 |  0:00:26s\nepoch 28 | loss: 0.66842 | val_0_logloss: 0.6671  |  0:00:27s\nepoch 29 | loss: 0.66797 | val_0_logloss: 0.66667 |  0:00:28s\nepoch 30 | loss: 0.66962 | val_0_logloss: 0.6669  |  0:00:29s\nepoch 31 | loss: 0.66672 | val_0_logloss: 0.67153 |  0:00:30s\nepoch 32 | loss: 0.6687  | val_0_logloss: 0.66886 |  0:00:31s\nepoch 33 | loss: 0.66679 | val_0_logloss: 0.66989 |  0:00:32s\nepoch 34 | loss: 0.66717 | val_0_logloss: 0.67193 |  0:00:33s\nepoch 35 | loss: 0.66713 | val_0_logloss: 0.66821 |  0:00:34s\nepoch 36 | loss: 0.66838 | val_0_logloss: 0.66799 |  0:00:35s\nepoch 37 | loss: 0.66747 | val_0_logloss: 0.67437 |  0:00:36s\n\nEarly stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_logloss = 0.66662\nBest weights from best epoch are automatically used!\n============ Fold 5 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.75471 | val_0_logloss: 0.80256 |  0:00:00s\nepoch 1  | loss: 0.68318 | val_0_logloss: 0.80937 |  0:00:02s\nepoch 2  | loss: 0.677   | val_0_logloss: 0.79899 |  0:00:03s\nepoch 3  | loss: 0.67988 | val_0_logloss: 0.78899 |  0:00:03s\nepoch 4  | loss: 0.67516 | val_0_logloss: 0.77782 |  0:00:04s\nepoch 5  | loss: 0.67127 | val_0_logloss: 0.77647 |  0:00:05s\nepoch 6  | loss: 0.67635 | val_0_logloss: 0.76761 |  0:00:06s\nepoch 7  | loss: 0.67478 | val_0_logloss: 0.74246 |  0:00:07s\nepoch 8  | loss: 0.67342 | val_0_logloss: 0.73709 |  0:00:08s\nepoch 9  | loss: 0.66931 | val_0_logloss: 0.73112 |  0:00:09s\nepoch 10 | loss: 0.66856 | val_0_logloss: 0.72667 |  0:00:10s\nepoch 11 | loss: 0.6706  | val_0_logloss: 0.71416 |  0:00:11s\nepoch 12 | loss: 0.67071 | val_0_logloss: 0.71293 |  0:00:12s\nepoch 13 | loss: 0.67033 | val_0_logloss: 0.71088 |  0:00:13s\nepoch 14 | loss: 0.67197 | val_0_logloss: 0.70293 |  0:00:14s\nepoch 15 | loss: 0.67062 | val_0_logloss: 0.69723 |  0:00:15s\nepoch 16 | loss: 0.67052 | val_0_logloss: 0.69896 |  0:00:16s\nepoch 17 | loss: 0.67058 | val_0_logloss: 0.69131 |  0:00:16s\nepoch 18 | loss: 0.66908 | val_0_logloss: 0.69194 |  0:00:17s\nepoch 19 | loss: 0.6679  | val_0_logloss: 0.69613 |  0:00:18s\nepoch 20 | loss: 0.67029 | val_0_logloss: 0.69161 |  0:00:19s\nepoch 21 | loss: 0.66782 | val_0_logloss: 0.69123 |  0:00:21s\nepoch 22 | loss: 0.66892 | val_0_logloss: 0.69602 |  0:00:21s\nepoch 23 | loss: 0.66847 | val_0_logloss: 0.69128 |  0:00:22s\nepoch 24 | loss: 0.66783 | val_0_logloss: 0.69044 |  0:00:24s\nepoch 25 | loss: 0.66728 | val_0_logloss: 0.69052 |  0:00:24s\nepoch 26 | loss: 0.66757 | val_0_logloss: 0.69064 |  0:00:25s\nepoch 27 | loss: 0.669   | val_0_logloss: 0.69331 |  0:00:26s\nepoch 28 | loss: 0.66726 | val_0_logloss: 0.69421 |  0:00:27s\nepoch 29 | loss: 0.6666  | val_0_logloss: 0.69405 |  0:00:28s\nepoch 30 | loss: 0.66624 | val_0_logloss: 0.70073 |  0:00:29s\nepoch 31 | loss: 0.66678 | val_0_logloss: 0.69052 |  0:00:30s\nepoch 32 | loss: 0.66617 | val_0_logloss: 0.69607 |  0:00:31s\nepoch 33 | loss: 0.66817 | val_0_logloss: 0.68809 |  0:00:32s\nepoch 34 | loss: 0.66635 | val_0_logloss: 0.68927 |  0:00:33s\nepoch 35 | loss: 0.66459 | val_0_logloss: 0.695   |  0:00:34s\nepoch 36 | loss: 0.66384 | val_0_logloss: 0.69086 |  0:00:35s\nepoch 37 | loss: 0.66617 | val_0_logloss: 0.68752 |  0:00:36s\nepoch 38 | loss: 0.66503 | val_0_logloss: 0.69065 |  0:00:37s\nepoch 39 | loss: 0.66625 | val_0_logloss: 0.68825 |  0:00:38s\nepoch 40 | loss: 0.66579 | val_0_logloss: 0.68913 |  0:00:38s\nepoch 41 | loss: 0.66381 | val_0_logloss: 0.6939  |  0:00:39s\nepoch 42 | loss: 0.66543 | val_0_logloss: 0.68626 |  0:00:40s\nepoch 43 | loss: 0.66473 | val_0_logloss: 0.68971 |  0:00:41s\nepoch 44 | loss: 0.66318 | val_0_logloss: 0.69015 |  0:00:42s\nepoch 45 | loss: 0.66359 | val_0_logloss: 0.68946 |  0:00:43s\nepoch 46 | loss: 0.66379 | val_0_logloss: 0.69048 |  0:00:44s\nepoch 47 | loss: 0.663   | val_0_logloss: 0.69363 |  0:00:45s\nepoch 48 | loss: 0.66311 | val_0_logloss: 0.68813 |  0:00:46s\nepoch 49 | loss: 0.66358 | val_0_logloss: 0.68865 |  0:00:47s\nepoch 50 | loss: 0.66322 | val_0_logloss: 0.69161 |  0:00:48s\nepoch 51 | loss: 0.66346 | val_0_logloss: 0.69665 |  0:00:49s\nepoch 52 | loss: 0.66255 | val_0_logloss: 0.6915  |  0:00:50s\n\nEarly stopping occurred at epoch 52 with best_epoch = 42 and best_val_0_logloss = 0.68626\nBest weights from best epoch are automatically used!\n============ Fold 6 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.75249 | val_0_logloss: 0.7806  |  0:00:00s\nepoch 1  | loss: 0.68507 | val_0_logloss: 0.81164 |  0:00:01s\nepoch 2  | loss: 0.68232 | val_0_logloss: 0.80742 |  0:00:03s\nepoch 3  | loss: 0.68007 | val_0_logloss: 0.78865 |  0:00:04s\nepoch 4  | loss: 0.6768  | val_0_logloss: 0.79321 |  0:00:05s\nepoch 5  | loss: 0.6767  | val_0_logloss: 0.77771 |  0:00:06s\nepoch 6  | loss: 0.67572 | val_0_logloss: 0.7457  |  0:00:07s\nepoch 7  | loss: 0.6743  | val_0_logloss: 0.75094 |  0:00:08s\nepoch 8  | loss: 0.67251 | val_0_logloss: 0.73068 |  0:00:09s\nepoch 9  | loss: 0.67301 | val_0_logloss: 0.72269 |  0:00:10s\nepoch 10 | loss: 0.67287 | val_0_logloss: 0.72003 |  0:00:10s\nepoch 11 | loss: 0.67096 | val_0_logloss: 0.71264 |  0:00:11s\nepoch 12 | loss: 0.67454 | val_0_logloss: 0.70264 |  0:00:12s\nepoch 13 | loss: 0.67325 | val_0_logloss: 0.69038 |  0:00:13s\nepoch 14 | loss: 0.67153 | val_0_logloss: 0.68331 |  0:00:14s\nepoch 15 | loss: 0.67212 | val_0_logloss: 0.68339 |  0:00:15s\nepoch 16 | loss: 0.67097 | val_0_logloss: 0.68864 |  0:00:16s\nepoch 17 | loss: 0.67088 | val_0_logloss: 0.67159 |  0:00:17s\nepoch 18 | loss: 0.67018 | val_0_logloss: 0.68536 |  0:00:18s\nepoch 19 | loss: 0.66951 | val_0_logloss: 0.66964 |  0:00:19s\nepoch 20 | loss: 0.67035 | val_0_logloss: 0.67681 |  0:00:20s\nepoch 21 | loss: 0.67151 | val_0_logloss: 0.67296 |  0:00:21s\nepoch 22 | loss: 0.66997 | val_0_logloss: 0.69876 |  0:00:22s\nepoch 23 | loss: 0.6706  | val_0_logloss: 0.67367 |  0:00:23s\nepoch 24 | loss: 0.67011 | val_0_logloss: 0.67523 |  0:00:23s\nepoch 25 | loss: 0.67    | val_0_logloss: 0.67487 |  0:00:24s\nepoch 26 | loss: 0.66948 | val_0_logloss: 0.67494 |  0:00:25s\nepoch 27 | loss: 0.66818 | val_0_logloss: 0.67419 |  0:00:26s\nepoch 28 | loss: 0.66913 | val_0_logloss: 0.67485 |  0:00:27s\nepoch 29 | loss: 0.66903 | val_0_logloss: 0.67961 |  0:00:28s\n\nEarly stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_logloss = 0.66964\nBest weights from best epoch are automatically used!\n============ Fold 7 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.75242 | val_0_logloss: 0.79385 |  0:00:00s\nepoch 1  | loss: 0.68865 | val_0_logloss: 0.77568 |  0:00:01s\nepoch 2  | loss: 0.67939 | val_0_logloss: 0.77554 |  0:00:02s\nepoch 3  | loss: 0.67858 | val_0_logloss: 0.76159 |  0:00:03s\nepoch 4  | loss: 0.67404 | val_0_logloss: 0.76895 |  0:00:04s\nepoch 5  | loss: 0.67426 | val_0_logloss: 0.74723 |  0:00:05s\nepoch 6  | loss: 0.67395 | val_0_logloss: 0.74427 |  0:00:06s\nepoch 7  | loss: 0.67456 | val_0_logloss: 0.72171 |  0:00:07s\nepoch 8  | loss: 0.67303 | val_0_logloss: 0.72393 |  0:00:08s\nepoch 9  | loss: 0.67176 | val_0_logloss: 0.71519 |  0:00:09s\nepoch 10 | loss: 0.66969 | val_0_logloss: 0.71304 |  0:00:10s\nepoch 11 | loss: 0.67158 | val_0_logloss: 0.70897 |  0:00:11s\nepoch 12 | loss: 0.67033 | val_0_logloss: 0.70307 |  0:00:12s\nepoch 13 | loss: 0.67036 | val_0_logloss: 0.6902  |  0:00:13s\nepoch 14 | loss: 0.67129 | val_0_logloss: 0.68616 |  0:00:14s\nepoch 15 | loss: 0.6698  | val_0_logloss: 0.67964 |  0:00:15s\nepoch 16 | loss: 0.67055 | val_0_logloss: 0.685   |  0:00:16s\nepoch 17 | loss: 0.67155 | val_0_logloss: 0.68746 |  0:00:17s\nepoch 18 | loss: 0.66819 | val_0_logloss: 0.67614 |  0:00:18s\nepoch 19 | loss: 0.66941 | val_0_logloss: 0.67789 |  0:00:18s\nepoch 20 | loss: 0.66973 | val_0_logloss: 0.68102 |  0:00:20s\nepoch 21 | loss: 0.66953 | val_0_logloss: 0.67667 |  0:00:21s\nepoch 22 | loss: 0.66909 | val_0_logloss: 0.6794  |  0:00:21s\nepoch 23 | loss: 0.67077 | val_0_logloss: 0.68348 |  0:00:22s\nepoch 24 | loss: 0.66865 | val_0_logloss: 0.67516 |  0:00:23s\nepoch 25 | loss: 0.66766 | val_0_logloss: 0.67386 |  0:00:24s\nepoch 26 | loss: 0.66867 | val_0_logloss: 0.67519 |  0:00:25s\nepoch 27 | loss: 0.66714 | val_0_logloss: 0.68461 |  0:00:26s\nepoch 28 | loss: 0.66817 | val_0_logloss: 0.68027 |  0:00:27s\nepoch 29 | loss: 0.66902 | val_0_logloss: 0.67629 |  0:00:28s\nepoch 30 | loss: 0.66634 | val_0_logloss: 0.67468 |  0:00:29s\nepoch 31 | loss: 0.66813 | val_0_logloss: 0.68112 |  0:00:30s\nepoch 32 | loss: 0.66851 | val_0_logloss: 0.68093 |  0:00:31s\nepoch 33 | loss: 0.6668  | val_0_logloss: 0.67535 |  0:00:32s\nepoch 34 | loss: 0.66563 | val_0_logloss: 0.67731 |  0:00:33s\nepoch 35 | loss: 0.66785 | val_0_logloss: 0.67708 |  0:00:34s\n\nEarly stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_logloss = 0.67386\nBest weights from best epoch are automatically used!\n============ Fold 8 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.75063 | val_0_logloss: 0.79873 |  0:00:00s\nepoch 1  | loss: 0.68406 | val_0_logloss: 0.79234 |  0:00:01s\nepoch 2  | loss: 0.67943 | val_0_logloss: 0.77788 |  0:00:02s\nepoch 3  | loss: 0.67565 | val_0_logloss: 0.77281 |  0:00:03s\nepoch 4  | loss: 0.67845 | val_0_logloss: 0.75539 |  0:00:04s\nepoch 5  | loss: 0.67766 | val_0_logloss: 0.75673 |  0:00:06s\nepoch 6  | loss: 0.67378 | val_0_logloss: 0.74201 |  0:00:06s\nepoch 7  | loss: 0.67431 | val_0_logloss: 0.73752 |  0:00:08s\nepoch 8  | loss: 0.67337 | val_0_logloss: 0.73    |  0:00:08s\nepoch 9  | loss: 0.67481 | val_0_logloss: 0.7165  |  0:00:09s\nepoch 10 | loss: 0.67137 | val_0_logloss: 0.7106  |  0:00:10s\nepoch 11 | loss: 0.67237 | val_0_logloss: 0.71089 |  0:00:11s\nepoch 12 | loss: 0.67309 | val_0_logloss: 0.69864 |  0:00:12s\nepoch 13 | loss: 0.67148 | val_0_logloss: 0.70366 |  0:00:13s\nepoch 14 | loss: 0.67186 | val_0_logloss: 0.6932  |  0:00:14s\nepoch 15 | loss: 0.67163 | val_0_logloss: 0.69077 |  0:00:15s\nepoch 16 | loss: 0.67169 | val_0_logloss: 0.69531 |  0:00:16s\nepoch 17 | loss: 0.67254 | val_0_logloss: 0.68191 |  0:00:17s\nepoch 18 | loss: 0.67119 | val_0_logloss: 0.67957 |  0:00:18s\nepoch 19 | loss: 0.67071 | val_0_logloss: 0.67638 |  0:00:19s\nepoch 20 | loss: 0.67236 | val_0_logloss: 0.67679 |  0:00:20s\nepoch 21 | loss: 0.67056 | val_0_logloss: 0.67841 |  0:00:21s\nepoch 22 | loss: 0.67002 | val_0_logloss: 0.67787 |  0:00:22s\nepoch 23 | loss: 0.6694  | val_0_logloss: 0.67345 |  0:00:22s\nepoch 24 | loss: 0.66732 | val_0_logloss: 0.67618 |  0:00:23s\nepoch 25 | loss: 0.67081 | val_0_logloss: 0.6743  |  0:00:24s\nepoch 26 | loss: 0.6712  | val_0_logloss: 0.67979 |  0:00:25s\nepoch 27 | loss: 0.66737 | val_0_logloss: 0.6838  |  0:00:26s\nepoch 28 | loss: 0.66911 | val_0_logloss: 0.67564 |  0:00:27s\nepoch 29 | loss: 0.66811 | val_0_logloss: 0.67745 |  0:00:28s\nepoch 30 | loss: 0.66607 | val_0_logloss: 0.67708 |  0:00:29s\nepoch 31 | loss: 0.66867 | val_0_logloss: 0.67637 |  0:00:30s\nepoch 32 | loss: 0.6678  | val_0_logloss: 0.67733 |  0:00:31s\nepoch 33 | loss: 0.66739 | val_0_logloss: 0.67934 |  0:00:32s\n\nEarly stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_logloss = 0.67345\nBest weights from best epoch are automatically used!\n============ Fold 9 ============\n\nDevice used : cuda\nepoch 0  | loss: 0.7427  | val_0_logloss: 0.80883 |  0:00:00s\nepoch 1  | loss: 0.68237 | val_0_logloss: 0.77888 |  0:00:01s\nepoch 2  | loss: 0.6817  | val_0_logloss: 0.7642  |  0:00:02s\nepoch 3  | loss: 0.67734 | val_0_logloss: 0.76659 |  0:00:03s\nepoch 4  | loss: 0.67485 | val_0_logloss: 0.76513 |  0:00:05s\nepoch 5  | loss: 0.67852 | val_0_logloss: 0.74085 |  0:00:05s\nepoch 6  | loss: 0.67776 | val_0_logloss: 0.75434 |  0:00:07s\nepoch 7  | loss: 0.6729  | val_0_logloss: 0.73803 |  0:00:08s\nepoch 8  | loss: 0.6739  | val_0_logloss: 0.72908 |  0:00:08s\nepoch 9  | loss: 0.67346 | val_0_logloss: 0.70952 |  0:00:09s\nepoch 10 | loss: 0.67192 | val_0_logloss: 0.71826 |  0:00:10s\nepoch 11 | loss: 0.67151 | val_0_logloss: 0.70568 |  0:00:11s\nepoch 12 | loss: 0.67138 | val_0_logloss: 0.70277 |  0:00:12s\nepoch 13 | loss: 0.67232 | val_0_logloss: 0.69886 |  0:00:13s\nepoch 14 | loss: 0.67268 | val_0_logloss: 0.69689 |  0:00:14s\nepoch 15 | loss: 0.67246 | val_0_logloss: 0.68731 |  0:00:15s\nepoch 16 | loss: 0.66953 | val_0_logloss: 0.68503 |  0:00:16s\nepoch 17 | loss: 0.66987 | val_0_logloss: 0.68124 |  0:00:17s\nepoch 18 | loss: 0.6719  | val_0_logloss: 0.68172 |  0:00:18s\nepoch 19 | loss: 0.67004 | val_0_logloss: 0.68126 |  0:00:19s\nepoch 20 | loss: 0.66967 | val_0_logloss: 0.67884 |  0:00:20s\nepoch 21 | loss: 0.67229 | val_0_logloss: 0.68112 |  0:00:21s\nepoch 22 | loss: 0.67083 | val_0_logloss: 0.67929 |  0:00:22s\nepoch 23 | loss: 0.66994 | val_0_logloss: 0.67939 |  0:00:22s\nepoch 24 | loss: 0.66893 | val_0_logloss: 0.67922 |  0:00:23s\nepoch 25 | loss: 0.66646 | val_0_logloss: 0.6792  |  0:00:24s\nepoch 26 | loss: 0.66861 | val_0_logloss: 0.67912 |  0:00:25s\nepoch 27 | loss: 0.66736 | val_0_logloss: 0.6803  |  0:00:26s\nepoch 28 | loss: 0.66613 | val_0_logloss: 0.67689 |  0:00:27s\nepoch 29 | loss: 0.66835 | val_0_logloss: 0.67682 |  0:00:28s\nepoch 30 | loss: 0.66656 | val_0_logloss: 0.67592 |  0:00:29s\nepoch 31 | loss: 0.66774 | val_0_logloss: 0.67765 |  0:00:30s\nepoch 32 | loss: 0.66817 | val_0_logloss: 0.68074 |  0:00:31s\nepoch 33 | loss: 0.66702 | val_0_logloss: 0.67596 |  0:00:32s\nepoch 34 | loss: 0.66663 | val_0_logloss: 0.68378 |  0:00:33s\nepoch 35 | loss: 0.66583 | val_0_logloss: 0.67825 |  0:00:34s\nepoch 36 | loss: 0.6674  | val_0_logloss: 0.67647 |  0:00:35s\nepoch 37 | loss: 0.66583 | val_0_logloss: 0.67801 |  0:00:36s\nepoch 38 | loss: 0.66603 | val_0_logloss: 0.67776 |  0:00:36s\nepoch 39 | loss: 0.66502 | val_0_logloss: 0.67785 |  0:00:37s\nepoch 40 | loss: 0.66506 | val_0_logloss: 0.68004 |  0:00:39s\n\nEarly stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_logloss = 0.67592\nBest weights from best epoch are automatically used!\nLog Loss Score: 0.67069\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/predictcreditcarddelinquency/sample_submission.csv\")\nsubmission.iloc[:, 1:] = net_preds\nsubmission.to_csv(\"meta_ensemble_submit.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T04:59:40.594637Z","iopub.execute_input":"2021-05-24T04:59:40.595249Z","iopub.status.idle":"2021-05-24T04:59:40.737394Z","shell.execute_reply.started":"2021-05-24T04:59:40.595204Z","shell.execute_reply":"2021-05-24T04:59:40.736550Z"},"trusted":true},"execution_count":16,"outputs":[]}]}