{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 설명 및 대회 전략\n",
    "+ GitHub README에 다 적어놨습니다.\n",
    "+ 중복데이터 처리를 어떻게 해야할지 고민을 하였습니다.\n",
    "+ Feature Engineering보단 Catboost의 Categorical 데이터에 관한 하이퍼파라미터 설정에 포커스를 맞췄습니다.\n",
    "+ [GitHub](https://github.com/ds-wook/PredictCreditCardDelinquency)\n",
    "\n",
    "# 참고한 자료\n",
    "+ [Hayo 님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2519?page=1&dtype=recent)\n",
    "+ [rollcake님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2526?page=1&dtype=recent)\n",
    "+ [datu님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2515?page=2&dtype=recent)\n",
    "+ [catboost 논문](https://arxiv.org/pdf/1706.09516.pdf)\n",
    "+ [catboost 활용 논문](https://arxiv.org/abs/2104.07553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:17:27.333498Z",
     "iopub.status.busy": "2021-05-24T03:17:27.333131Z",
     "iopub.status.idle": "2021-05-24T03:17:57.518299Z",
     "shell.execute_reply": "2021-05-24T03:17:57.517366Z",
     "shell.execute_reply.started": "2021-05-24T03:17:27.333420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_tabnet\n",
      "  Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to c:\\users\\public\\documents\\estsoft\\creatortemp\\pip-install-t1uratdz\\pytorch-tabnet\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied, skipping upgrade: scikit_learn>0.21 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (0.21.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>1.4 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (1.5.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.17 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0,>=4.36 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (4.61.0)\n",
      "Requirement already satisfied, skipping upgrade: torch<2.0,>=1.2 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch_tabnet) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (3.10.0.0)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (0.8)\n",
      "Building wheels for collected packages: pytorch-tabnet\n",
      "  Building wheel for pytorch-tabnet (PEP 517): started\n",
      "  Building wheel for pytorch-tabnet (PEP 517): finished with status 'done'\n",
      "  Created wheel for pytorch-tabnet: filename=pytorch_tabnet-3.1.1-py3-none-any.whl size=39905 sha256=9ccc35e67f1f2d9ef4ad019987dade1349d2007168da94d2c4855d338cc8d7b6\n",
      "  Stored in directory: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-ephem-wheel-cache-w8hmpgov\\wheels\\00\\61\\ad\\ba7b2cff3afdf0971f6205bd564b90aaab96bf84c8cc36ef9e\n",
      "Successfully built pytorch-tabnet\n",
      "Installing collected packages: pytorch-tabnet\n",
      "  Attempting uninstall: pytorch-tabnet\n",
      "    Found existing installation: pytorch-tabnet 3.1.1\n",
      "    Uninstalling pytorch-tabnet-3.1.1:\n",
      "      Successfully uninstalled pytorch-tabnet-3.1.1\n",
      "Successfully installed pytorch-tabnet-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y typing # this should avoid  AttributeError: type object 'Callable' has no attribute '_abc_registry'\n",
    "!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (0.26)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from catboost) (1.11.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from catboost) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from catboost) (1.19.5)\n",
      "Requirement already satisfied: plotly in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from catboost) (4.14.3)\n",
      "Requirement already satisfied: graphviz in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from catboost) (0.16)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from catboost) (0.24.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.7.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2018.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.0.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from plotly->catboost) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (39.1.0)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.2.1-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: wheel in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from lightgbm) (0.31.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from lightgbm) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from lightgbm) (0.21.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hwang in beom\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.13.2)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-24T03:17:57.522016Z",
     "iopub.status.busy": "2021-05-24T03:17:57.521745Z",
     "iopub.status.idle": "2021-05-24T03:18:01.889106Z",
     "shell.execute_reply": "2021-05-24T03:18:01.888260Z",
     "shell.execute_reply.started": "2021-05-24T03:17:57.521975Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hwang in beom\\Anaconda3\\lib\\site-packages\\distributed\\config.py:63: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config.update(yaml.load(text) or {})\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Union, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "+ catboost가 이 대회의 핵심 알고리즘이라고 생각하여 categorical한 변수를 만드는게 중요하다고 생각하여 따로 함수를 만듬\n",
    "+ 다른 알고리즘들은 categorical한 변수보다 numeric한 변수 처리에 초점을 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:01.891168Z",
     "iopub.status.busy": "2021-05-24T03:18:01.890877Z",
     "iopub.status.idle": "2021-05-24T03:18:01.947487Z",
     "shell.execute_reply": "2021-05-24T03:18:01.946520Z",
     "shell.execute_reply.started": "2021-05-24T03:18:01.891140Z"
    }
   },
   "outputs": [],
   "source": [
    "def category_income(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    2.객체간의 비교 연산 메소드\n",
    "      eq() == 같다 / ne() =! 다르다 / lt() < 작다 / gt() > 크다 / le() < 작거나 같다 / ge() >= 크거나 같다\n",
    "      기호와 메서드의 차이 => 메서드에서 제공하는 옵션을 사용해서 좀 더 정교한 연산을 수행한다.\n",
    "    \"\"\"\n",
    "    \n",
    "    data[\"income_total\"] = data[\"income_total\"] / 10000\n",
    "    conditions = [\n",
    "        (data[\"income_total\"].le(18)),\n",
    "        (data[\"income_total\"].gt(18) & data[\"income_total\"].le(33)),\n",
    "        (data[\"income_total\"].gt(33) & data[\"income_total\"].le(49)),\n",
    "        (data[\"income_total\"].gt(49) & data[\"income_total\"].le(64)),\n",
    "        (data[\"income_total\"].gt(64) & data[\"income_total\"].le(80)),\n",
    "        (data[\"income_total\"].gt(80) & data[\"income_total\"].le(95)),\n",
    "        (data[\"income_total\"].gt(95) & data[\"income_total\"].le(111)),\n",
    "        (data[\"income_total\"].gt(111) & data[\"income_total\"].le(126)),\n",
    "        (data[\"income_total\"].gt(126) & data[\"income_total\"].le(142)),\n",
    "        (data[\"income_total\"].gt(142)),\n",
    "    ]\n",
    "    choices = [i for i in range(10)]\n",
    "\n",
    "    data[\"income_total\"] = np.select(conditions, choices)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    path = \"C:/Users/hwang in beom/Desktop/data/1.신용카드 연체/\"\n",
    "    train = pd.read_csv(path + \"train.csv\")\n",
    "    train = train.drop([\"index\"], axis=1)\n",
    "    train.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "    test = pd.read_csv(path + \"test.csv\")\n",
    "    test = test.drop([\"index\"], axis=1)\n",
    "    test.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "    \"\"\"\n",
    "    3.절대값  + 음수 제거\n",
    "    \"\"\"\n",
    "    # absolute \n",
    "    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n",
    "    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n",
    "    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n",
    "    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n",
    "    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n",
    "    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n",
    "\n",
    "    \"\"\"\n",
    "    4.파생 변수 생성 - DAYS_BIRTH 값 month / week \n",
    "    \"\"\"\n",
    "    # DAYS_BIRTH\n",
    "    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    5.Age 변수 날짜를 나이로 변환  \n",
    "    \"\"\"\n",
    "    # Age\n",
    "    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n",
    "    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    6.DAYS_EMPLOYED_month 변수 -> month / week 라는 파생변수 생성\n",
    "    \"\"\"\n",
    "    # DAYS_EMPLOYED\n",
    "    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    7.EMPLOYED 는 연도로 나눔\n",
    "    \"\"\"\n",
    "    # EMPLOYED\n",
    "    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n",
    "    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    8.before_EMPLOYED 라는 파생 변수 생성 \n",
    "    태어난 날짜 - 일한날짜 = 일을 안했던 기간을 산정\n",
    "    그후 이걸 month와 week라는 파생변수 생성\n",
    "    \"\"\"\n",
    "    # before_EMPLOYED\n",
    "    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n",
    "    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n",
    "    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n",
    "    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n",
    "    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n",
    "    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n",
    "\n",
    "    \"\"\"\n",
    "    9.user_code 생성 \n",
    "    gender + car + reality \n",
    "    \"\"\"\n",
    "    \n",
    "    # gender_car_reality\n",
    "    train[\"user_code\"] = (\n",
    "        train[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"reality\"].astype(str)\n",
    "    )\n",
    "    test[\"user_code\"] = (\n",
    "        test[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"reality\"].astype(str)\n",
    "    )\n",
    "\n",
    "    del_cols = [\n",
    "        \"gender\",\n",
    "        \"car\",\n",
    "        \"reality\",\n",
    "        \"email\",\n",
    "        \"child_num\",\n",
    "        \"DAYS_BIRTH\",\n",
    "        \"DAYS_EMPLOYED\",\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    10.family_size 이상치 처리\n",
    "    \"\"\"\n",
    "    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n",
    "    train.drop(del_cols, axis=1, inplace=True)\n",
    "    test.drop(del_cols, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = [\n",
    "        \"income_type\",\n",
    "        \"edu_type\",\n",
    "        \"family_type\",\n",
    "        \"house_type\",\n",
    "        \"occyp_type\",\n",
    "        \"user_code\",\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    11.label encoder \n",
    "    \"\"\"\n",
    "    for col in cat_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(train[col])\n",
    "        train[col] = label_encoder.transform(train[col])\n",
    "        test[col] = label_encoder.transform(test[col])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def cat_load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "    # 1. finllna 및 필요없는 index 삭제\n",
    "    path = \"C:/Users/hwang in beom/Desktop/data/1.신용카드 연체/\"\n",
    "    train = pd.read_csv(path + \"train.csv\")\n",
    "    train = train.drop([\"index\"], axis=1)\n",
    "    train.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "    test = pd.read_csv(path + \"test.csv\")\n",
    "    test = test.drop([\"index\"], axis=1)\n",
    "    test.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "\n",
    "# \t2. 절대값  + 음수 제거\n",
    "    # absolute\n",
    "    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n",
    "    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n",
    "    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n",
    "    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n",
    "    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n",
    "    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n",
    "\n",
    "    \n",
    "# 3. 객체간의 비교 연산 메소드\n",
    "#   eq() == 같다 / ne() =! 다르다 / lt() < 작다 / gt() > 크다 / le() < 작거나 같다 / ge() >=크거나 같다.\n",
    "#  기호와 메서드의 차이 => 메서드에서 제공하는 옵션을 사용해서 좀 더 정교한 연산을 수행한다.\n",
    "    # income_total\n",
    "    train = category_income(train)\n",
    "    test = category_income(test)\n",
    "\n",
    "    \n",
    "    \n",
    "# \t4. 파생 변수 생성 - DAYS_BIRTH 값 month / week\n",
    "\n",
    "    # DAYS_BIRTH\n",
    "    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n",
    "\n",
    "    \n",
    "# \t5. Age 변수 날짜를 나이로 변환  \n",
    "    # Age\n",
    "    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n",
    "    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n",
    "\n",
    "# \t6. DAYS_EMPLOYED_month 변수 -> month / week 라는 파생변수 생성\n",
    "    # DAYS_EMPLOYED\n",
    "    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "# \t7. EMPLOYED 는 연도로 나눔\n",
    "    # EMPLOYED\n",
    "    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n",
    "    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n",
    "\n",
    "    \n",
    "    \n",
    "# \t8. before_EMPLOYED 라는 파생 변수 생성\n",
    "# 태어난 날짜 - 일한날짜 = 일을 안했던 기간을 산정\n",
    "#  그 후 이걸 month와 week라는 파생변수 생성\n",
    "    # before_EMPLOYED \n",
    "    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n",
    "    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n",
    "    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n",
    "    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n",
    "    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n",
    "    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n",
    "\n",
    "    \n",
    "# \t9. user_code 생성\n",
    "#gender + car + reality\n",
    "\n",
    "    # gender_car_reality\n",
    "    train[\"user_code\"] = (\n",
    "        train[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"reality\"].astype(str)\n",
    "    )\n",
    "    test[\"user_code\"] = (\n",
    "        test[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"reality\"].astype(str)\n",
    "    )\n",
    "\n",
    "    del_cols = [\n",
    "        \"gender\",\n",
    "        \"car\",\n",
    "        \"reality\",\n",
    "        \"email\",\n",
    "        \"child_num\",\n",
    "        \"DAYS_BIRTH\",\n",
    "        \"DAYS_EMPLOYED\",\n",
    "    ]\n",
    "\n",
    "# \t11. family_size 이상치 처리\n",
    "    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n",
    "    train.drop(del_cols, axis=1, inplace=True)\n",
    "    test.drop(del_cols, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = [\n",
    "        \"income_type\",\n",
    "        \"edu_type\",\n",
    "        \"family_type\",\n",
    "        \"house_type\",\n",
    "        \"occyp_type\",\n",
    "        \"user_code\",\n",
    "    ]\n",
    "    \n",
    "    \n",
    "\n",
    "# \t12. label encoder\n",
    "    for col in cat_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(train[col])\n",
    "        train[col] = label_encoder.transform(train[col])\n",
    "        test[col] = label_encoder.transform(test[col])\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:01.949594Z",
     "iopub.status.busy": "2021-05-24T03:18:01.949262Z",
     "iopub.status.idle": "2021-05-24T03:18:01.977430Z",
     "shell.execute_reply": "2021-05-24T03:18:01.976562Z",
     "shell.execute_reply.started": "2021-05-24T03:18:01.949559Z"
    }
   },
   "outputs": [],
   "source": [
    "# 함수받는 형태가 신기헀다. 이렇게 강제로 받을 수 있는 것 같다.\n",
    "\n",
    "# Catboost\n",
    "def stratified_kfold_cat(\n",
    "    params: Dict[str, Union[int, float, str, List[str]]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "    #StratifiedKFold 사용\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    \n",
    "    #데이터 프레임 만드는 작업(0으로 채워넣어서 만드는 작업)\n",
    "    cat_oof = np.zeros((X.shape[0], 3))\n",
    "    cat_preds = np.zeros((X_test.shape[0], 3))\n",
    "    cat_cols = [c for c in X.columns if X[c].dtypes == \"int64\"]\n",
    "\n",
    "    \n",
    "    cat_cols\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n",
    "        valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "\n",
    "        # 100번까지만 돌리고 가장 좋은 모델 true\n",
    "        model.fit(\n",
    "            train_data,\n",
    "            eval_set=valid_data,\n",
    "            early_stopping_rounds=100,\n",
    "            use_best_model=True,\n",
    "            verbose=100,\n",
    "        )\n",
    "\n",
    "        cat_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        cat_preds += model.predict_proba(X_test) / n_fold\n",
    "\n",
    "    log_score = log_loss(y, cat_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\\n\")\n",
    "    return cat_oof, cat_preds\n",
    "\n",
    "\n",
    "# Light GBM\n",
    "def stratified_kfold_lgbm(\n",
    "    params: Dict[str, Union[int, float, str]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    lgb_oof = np.zeros((X.shape[0], 3))\n",
    "    lgb_preds = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        pre_model = LGBMClassifier(**params)\n",
    "\n",
    "        pre_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100,\n",
    "        )\n",
    "        params2 = params.copy()\n",
    "        params2[\"learning_rate\"] = params[\"learning_rate\"] * 0.1\n",
    "\n",
    "        model = LGBMClassifier(**params2)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100,\n",
    "            init_model=pre_model,\n",
    "        )\n",
    "        lgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        lgb_preds += model.predict_proba(X_test) / n_fold\n",
    "\n",
    "    log_score = log_loss(y, lgb_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
    "\n",
    "    return lgb_oof, lgb_preds\n",
    "\n",
    "\n",
    "# XGB\n",
    "def stratified_kfold_xgb(\n",
    "    params: Dict[str, Union[int, float, str]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    xgb_oof = np.zeros((X.shape[0], 3))\n",
    "    xgb_preds = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100,\n",
    "        )\n",
    "\n",
    "        xgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        xgb_preds += model.predict_proba(X_test) / n_fold\n",
    "\n",
    "    log_score = log_loss(y, xgb_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
    "\n",
    "    return xgb_oof, xgb_preds\n",
    "\n",
    "\n",
    "# Random Foreset\n",
    "def stratified_kfold_rf(\n",
    "    params: Dict[str, Union[int, float, str, bool]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    rf_oof = np.zeros((X.shape[0], 3))\n",
    "    rf_preds = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "        )\n",
    "\n",
    "        rf_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        rf_preds += model.predict_proba(X_test) / n_fold\n",
    "        print(f\"Log Loss Score: {log_loss(y_valid, rf_oof[valid_idx]):.5f}\")\n",
    "\n",
    "    log_score = log_loss(y, rf_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
    "\n",
    "    return rf_oof, rf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Train\n",
    "+ catboost 같은 경우 categorical한 feature를 고정시켜주는게 핵심이라고 생각하여 하이퍼파라미터튜닝에 신경을 씀\n",
    "+ 하이퍼파라미터튜닝은 optuna 라이브러리로 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:01.979135Z",
     "iopub.status.busy": "2021-05-24T03:18:01.978709Z",
     "iopub.status.idle": "2021-05-24T03:18:02.424836Z",
     "shell.execute_reply": "2021-05-24T03:18:02.424030Z",
     "shell.execute_reply.started": "2021-05-24T03:18:01.979068Z"
    }
   },
   "outputs": [],
   "source": [
    "train_cat, test_cat = cat_load_dataset()\n",
    "X = train_cat.drop(\"credit\", axis=1)\n",
    "y = train_cat[\"credit\"]\n",
    "\n",
    "X_test = test_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:02.426466Z",
     "iopub.status.busy": "2021-05-24T03:18:02.426103Z",
     "iopub.status.idle": "2021-05-24T04:38:25.616125Z",
     "shell.execute_reply": "2021-05-24T04:38:25.615056Z",
     "shell.execute_reply.started": "2021-05-24T03:18:02.426429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Fold 0 ============\n",
      "\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "categorical features indices in the model are set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20] and train dataset categorical features indices are set to [5, 6, 7, 13, 17]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-19d41cbf91fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m }\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mcat_oof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstratified_kfold_cat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-1505ec2beb05>\u001b[0m in \u001b[0;36mstratified_kfold_cat\u001b[1;34m(params, n_fold, X, y, X_test)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0muse_best_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         )\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   4673\u001b[0m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0;32m   4674\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4675\u001b[1;33m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0m\u001b[0;32m   4676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   1984\u001b[0m             \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_snapshot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_snapshot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1985\u001b[0m             \u001b[0msnapshot_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msnapshot_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnapshot_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msnapshot_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1986\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1987\u001b[0m         )\n\u001b[0;32m   1988\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   1858\u001b[0m                           \"please pass it to Pool initialization and use Pool in fit\")\n\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m         \u001b[0mcat_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_feature_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cat_features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1861\u001b[0m         \u001b[0mtext_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_feature_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text_features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m         \u001b[0membedding_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_feature_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embedding_features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_process_feature_indices\u001b[1;34m(feature_indices, pool, params, param_name)\u001b[0m\n\u001b[0;32m   1810\u001b[0m                                 \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_indices_from_params\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m                                 \u001b[1;34m\" and train dataset \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfeature_type_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" features indices are set to \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m                                 str(feature_indices_from_pool))\n\u001b[0m\u001b[0;32m   1813\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeaturesData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m         raise CatBoostError(\n",
      "\u001b[1;31mCatBoostError\u001b[0m: categorical features indices in the model are set to [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20] and train dataset categorical features indices are set to [5, 6, 7, 13, 17]"
     ]
    }
   ],
   "source": [
    "cat_params = {\n",
    "    \"learning_rate\": 0.026612467217016746,\n",
    "    \"l2_leaf_reg\": 0.3753065117824262,\n",
    "    \"max_depth\": 8,\n",
    "    \"bagging_temperature\": 1,\n",
    "    \"min_data_in_leaf\": 57,\n",
    "    \"max_bin\": 494,\n",
    "    \"random_state\": 42,\n",
    "    \"eval_metric\": \"MultiClass\",\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 500,\n",
    "    \"iterations\": 10000,\n",
    "    \"cat_features\": [\n",
    "        \"income_total\",\n",
    "        \"income_type\",\n",
    "        \"edu_type\",\n",
    "        \"family_type\",\n",
    "        \"house_type\",\n",
    "        \"FLAG_MOBIL\",\n",
    "        \"work_phone\",\n",
    "        \"phone\",\n",
    "        \"occyp_type\",\n",
    "        \"begin_month\",\n",
    "        \"DAYS_BIRTH_month\",\n",
    "        \"DAYS_BIRTH_week\",\n",
    "        \"Age\",\n",
    "        \"DAYS_EMPLOYED_month\",\n",
    "        \"DAYS_EMPLOYED_week\",\n",
    "        \"before_EMPLOYED\",\n",
    "        \"before_EMPLOYED_month\",\n",
    "        \"before_EMPLOYED_week\",\n",
    "        \"user_code\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "cat_oof, cat_preds = stratified_kfold_cat(cat_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Train\n",
    "+ 하이퍼파라미터 튜닝은 optuna를 사용했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:38:25.618271Z",
     "iopub.status.busy": "2021-05-24T04:38:25.617672Z",
     "iopub.status.idle": "2021-05-24T04:38:25.929996Z",
     "shell.execute_reply": "2021-05-24T04:38:25.929038Z",
     "shell.execute_reply.started": "2021-05-24T04:38:25.618225Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = load_dataset()\n",
    "X = train.drop(\"credit\", axis=1)\n",
    "y = train[\"credit\"]\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:38:25.932882Z",
     "iopub.status.busy": "2021-05-24T04:38:25.932517Z",
     "iopub.status.idle": "2021-05-24T04:41:27.262827Z",
     "shell.execute_reply": "2021-05-24T04:41:27.261743Z",
     "shell.execute_reply.started": "2021-05-24T04:38:25.932843Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"reg_alpha\": 5.998770177220496e-05,\n",
    "    \"reg_lambda\": 0.07127674208132959,\n",
    "    \"max_depth\": 18,\n",
    "    \"num_leaves\": 125,\n",
    "    \"colsample_bytree\": 0.4241631237880101,\n",
    "    \"subsample\": 0.8876057928391585,\n",
    "    \"subsample_freq\": 5,\n",
    "    \"min_child_samples\": 5,\n",
    "    \"max_bin\": 449,\n",
    "    \"random_state\": 42,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "}\n",
    "lgbm_oof, lgbm_preds = stratified_kfold_lgbm(lgb_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Train\n",
    "+ 하이퍼파라미터 튜닝을 optuna를 사용했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:41:27.264935Z",
     "iopub.status.busy": "2021-05-24T04:41:27.264591Z",
     "iopub.status.idle": "2021-05-24T04:51:02.999834Z",
     "shell.execute_reply": "2021-05-24T04:51:02.999034Z",
     "shell.execute_reply.started": "2021-05-24T04:41:27.264897Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"eta\": 0.023839252347297356,\n",
    "    \"reg_alpha\": 6.99554614267605e-06,\n",
    "    \"reg_lambda\": 0.010419988953061583,\n",
    "    \"max_depth\": 15,\n",
    "    \"max_leaves\": 159,\n",
    "    \"colsample_bytree\": 0.4515469593932409,\n",
    "    \"subsample\": 0.7732694309118915,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"gamma\": 0.6847131315687576,\n",
    "    \"random_state\": 42,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "}\n",
    "xgb_oof, xgb_preds = stratified_kfold_xgb(xgb_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:51:03.001643Z",
     "iopub.status.busy": "2021-05-24T04:51:03.001315Z",
     "iopub.status.idle": "2021-05-24T04:52:28.223881Z",
     "shell.execute_reply": "2021-05-24T04:52:28.223049Z",
     "shell.execute_reply.started": "2021-05-24T04:51:03.001606Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "        \"criterion\": \"gini\",\n",
    "        \"n_estimators\": 300,\n",
    "        \"min_samples_split\": 10,\n",
    "        \"min_samples_leaf\": 2,\n",
    "        \"max_features\": \"auto\",\n",
    "        \"oob_score\": True,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "rf_oof, rf_preds = stratified_kfold_rf(rf_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.225718Z",
     "iopub.status.busy": "2021-05-24T04:52:28.225231Z",
     "iopub.status.idle": "2021-05-24T04:52:28.633099Z",
     "shell.execute_reply": "2021-05-24T04:52:28.632139Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.225678Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = load_dataset()\n",
    "train_x = train.drop(\"credit\", axis = 1)\n",
    "train_y = train['credit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.634938Z",
     "iopub.status.busy": "2021-05-24T04:52:28.634352Z",
     "iopub.status.idle": "2021-05-24T04:52:28.642952Z",
     "shell.execute_reply": "2021-05-24T04:52:28.641787Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.634896Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = np.concatenate([cat_oof, lgbm_oof, xgb_oof, rf_oof], axis=1)\n",
    "train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.645045Z",
     "iopub.status.busy": "2021-05-24T04:52:28.644375Z",
     "iopub.status.idle": "2021-05-24T04:52:28.652160Z",
     "shell.execute_reply": "2021-05-24T04:52:28.651111Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.644976Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred = np.concatenate([cat_preds, lgbm_preds, xgb_preds, rf_preds], axis=1)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tabular\n",
    "+ Stacking Ensemble을 사용하며 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.654142Z",
     "iopub.status.busy": "2021-05-24T04:52:28.653618Z",
     "iopub.status.idle": "2021-05-24T04:52:28.659053Z",
     "shell.execute_reply": "2021-05-24T04:52:28.658271Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.654109Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.660957Z",
     "iopub.status.busy": "2021-05-24T04:52:28.660315Z",
     "iopub.status.idle": "2021-05-24T04:59:40.592964Z",
     "shell.execute_reply": "2021-05-24T04:59:40.591970Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.660920Z"
    }
   },
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "splits = folds.split(train_pred, train_y)\n",
    "net_oof = np.zeros((train_pred.shape[0], 3))\n",
    "net_preds = np.zeros((test_pred.shape[0], 3))\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(f\"============ Fold {fold} ============\\n\")\n",
    "    X_train, X_valid = train_pred[train_idx], train_pred[valid_idx]\n",
    "    y_train, y_valid = train_y[train_idx], train_y[valid_idx]\n",
    "    model = TabNetMultiTaskClassifier(\n",
    "            n_d=64, n_a=64, n_steps=1,\n",
    "            lambda_sparse=1e-4,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_params = {\"gamma\": 0.9, \"step_size\": 50},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            mask_type=\"entmax\", \n",
    "            device_name=device\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train.reshape(-1,1),\n",
    "        eval_set=[(X_valid, y_valid.reshape(-1,1))],\n",
    "        max_epochs=100,\n",
    "        batch_size=1024,\n",
    "        eval_metric=[\"logloss\"],\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=1,\n",
    "        drop_last=False\n",
    "    )\n",
    "    net_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "    net_preds += model.predict_proba(test_pred)[0] / n_fold\n",
    "log_score = log_loss(train_y, net_oof)\n",
    "print(f\"Log Loss Score: {log_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:59:40.595249Z",
     "iopub.status.busy": "2021-05-24T04:59:40.594637Z",
     "iopub.status.idle": "2021-05-24T04:59:40.737394Z",
     "shell.execute_reply": "2021-05-24T04:59:40.736550Z",
     "shell.execute_reply.started": "2021-05-24T04:59:40.595204Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/predictcreditcarddelinquency/sample_submission.csv\")\n",
    "submission.iloc[:, 1:] = net_preds\n",
    "submission.to_csv(\"meta_ensemble_submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
